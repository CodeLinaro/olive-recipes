{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMET Quantization workflow for Gemma3-4B Language Model\n",
    "\n",
    "This notebook shows a working code example of how to use AIMET to quantize Gemma3-4B Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Required packages\n",
    "The notebook assumes AIMET and Gemma3 related packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != '__main__':\n",
    "    raise Exception(\"Killing multiprocessing spawn started by Converter during model preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages only if running in jupyter notebook mode\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    !sudo -H apt-get -qq update\n",
    "    !sudo -H apt-get -qq install libc++-dev\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir transformers==4.50.0\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir tokenizers==0.21.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow\n",
    "This notebook covers the following\n",
    "1. Setting QNN SDK and NSP target\n",
    "2. Instantiate and evaluate HuggingFace model\n",
    "3. Instantiate and adapt FP32 model\n",
    "4. Model Sample Input\n",
    "5. Prepare model using QAIRT model preparer pro\n",
    "6. Evaluation of prepared model\n",
    "7. Quantization \n",
    "8. Export\n",
    "\n",
    "\n",
    "### What this notebook is not \n",
    "* This notebook is not intended to show the full scope of optimization. For example, the flow will not use QAT, KD-QAT as deliberate choice to have the notebook execute more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting QNN SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "\n",
    "QNN_SDK_ROOT = \"/tmp/qnn\"\n",
    "assert QNN_SDK_ROOT is not None, 'Please point the QNN_SDK_ROOT variable to your QNN SDK'\n",
    "assert os.path.exists(QNN_SDK_ROOT), \"QNN_SDK_ROOT doesn't exist!\"\n",
    "sys.path.insert(0, QNN_SDK_ROOT + '/lib/python')\n",
    "\n",
    "lib_clang_path = os.path.join(QNN_SDK_ROOT, 'lib', 'x86_64-linux-clang')\n",
    "LD_LIBRARY_PATH = os.getenv('LD_LIBRARY_PATH', None)\n",
    "os.environ['LD_LIBRARY_PATH'] = lib_clang_path + ':' + LD_LIBRARY_PATH if LD_LIBRARY_PATH is not None else lib_clang_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setting NSP Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select quantsim config based on target\n",
    "htp_config_file =  \"htp_quantsim_config_v81.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate and evaluate HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.common.debug.recipe_logger import recipe_dump_init\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_env_info\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoProcessor\n",
    "\n",
    "#======================Configurable setting by users================================\n",
    "run_ppl_eval = True\n",
    "load_optimized_weights = True   # load spinquant checkpoint from spinquant notebook for optimized quantization accuracy\n",
    "cache_dir='/tmp/cache_dir'\n",
    "output_dir = '/tmp/output_dir'  # point to where the export artifacts of this notebook to be saved\n",
    "\n",
    "model_name = 'gemma_4b'\n",
    "model_id=\"google/gemma-3-4b-it\"\n",
    "if load_optimized_weights:\n",
    "    optimized_model_id = \"/tmp/output_dir/spinquant\"\n",
    "\n",
    "lmm_config = AutoConfig.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "context_length = 8192\n",
    "# To help with debugging num_hidden_layers could be set to 6 to quickly verify the pipeline\n",
    "num_hidden_layers = int(os.getenv(\"NUM_HIDDEN_LAYERS\", 0))\n",
    "lmm_config.text_config.num_hidden_layers = num_hidden_layers if num_hidden_layers > 0 else lmm_config.text_config.num_hidden_layers\n",
    "\n",
    "print(f'num_layer: {lmm_config.text_config.num_hidden_layers}, context_length: {context_length},'\n",
    "      f'num_attention_heads :{lmm_config.text_config.num_attention_heads},  num_kv_heads: {lmm_config.text_config.num_key_value_heads}')\n",
    "\n",
    "# Recipe_logger: Initialize the logger and log environment details \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "recipe_dump_init(output_dir)\n",
    "\n",
    "llm_lib_log_env_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Instantiate the HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.gemma3 import modeling_gemma3\n",
    "from genai_lib.common.debug.profiler import event_marker\n",
    "\n",
    "with event_marker('Load FP model'):\n",
    "    model = modeling_gemma3.Gemma3ForConditionalGeneration.from_pretrained(model_id, config=lmm_config, cache_dir=cache_dir)\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = '0'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, use_fast=True, trust_remote_code=True)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "    ## Adjust the tokenizer to limit to context_length\n",
    "    tokenizer.model_max_length = context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Instantiate Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.wikitext_dataloader import get_wiki_dataset\n",
    "from llm_utils.llava_dataloader import get_llava_dataset\n",
    "\n",
    "train_dataloader, test_dataloader, _ = get_wiki_dataset(context_length, tokenizer, cache_dir=cache_dir)\n",
    "\n",
    "dataset_path = \"<path to folder containing the coco dataset root folder>\"\n",
    "data_files = \"llm_utils/llava_dataset/llava_v1_5_mix665k_300.json\"\n",
    "llava_dataset = get_llava_dataset(tokenizer, processor, data_files=data_files, dataset_path=dataset_path, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 HuggingFace FP model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.utils import place_model\n",
    "from genai_lib.llm.evaluation_utils import llm_evaluate_ppl_with_dataloader\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_property, Property\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_metric, ModelType, Metric\n",
    "\n",
    "\n",
    "# Recipe_logger: Log the context_length property and the metrics.\n",
    "llm_lib_log_property({Property.context_length : context_length})\n",
    "\n",
    "if run_ppl_eval:\n",
    "    with event_marker(\"HuggingFace FP model eval\"):\n",
    "        with place_model(model, torch.device('cuda')):\n",
    "            orig_ppl = llm_evaluate_ppl_with_dataloader(model=model.language_model, dataloader=test_dataloader)\n",
    "\n",
    "    llm_lib_log_metric(ModelType.hf_model, Metric.ppl, orig_ppl, model_name=\"base\")\n",
    "    print(f\"PPL score of HuggingFace FP model = {orig_ppl}\")\n",
    "\n",
    "# Remove the HuggingFace model from memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate and adapt FP32 model\n",
    "\n",
    "#### 3.1 Adapt FP32 model definition for inference on HTP.\n",
    "- The following adaptations are done to replace default attention module with attention definition that compatible with NSP backend\n",
    "  * use conv instead of linear for Q,K,V,O projections\n",
    "  * bypass attention and causal mask generation and replace with pre-generated 2D-mask input\n",
    "  * output only newly created V and transposed K instead of entire augmented KV sequence\n",
    "  * input pre-calculated positional embedding instead of position ids, thus bypass the embedding generation in the model\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import cache_utils\n",
    "from transformers.models.gemma3 import modeling_gemma3\n",
    "from gemma3.adaptation import (\n",
    "    Gemma3Attention,\n",
    "    Gemma3DecoderLayer,\n",
    "    Gemma3TextModel,\n",
    "    Gemma3ForCausalLM,\n",
    "    adapted_update_causal_mask,\n",
    "    DynamicCache_update,\n",
    "    DynamicCache_get_seq_length,\n",
    "    update_attr\n",
    ")\n",
    "\n",
    "with event_marker(\"FP model adaptation configuration\"):\n",
    "    modeling_gemma3.Gemma3TextModel = Gemma3TextModel\n",
    "    modeling_gemma3.Gemma3ForCausalLM = Gemma3ForCausalLM\n",
    "    modeling_gemma3.Gemma3DecoderLayer = Gemma3DecoderLayer\n",
    "    modeling_gemma3.Gemma3Attention = Gemma3Attention\n",
    "\n",
    "    # Bypass attention_mask preparation\n",
    "    assert hasattr(modeling_gemma3.Gemma3TextModel, '_update_causal_mask'), \\\n",
    "        \"GaussModel does not have _update_causal_mask as attribute\"\n",
    "    modeling_gemma3.Gemma3TextModel._update_causal_mask = adapted_update_causal_mask\n",
    "    \n",
    "    # Adapting KV$ management\n",
    "    assert update_attr(cache_utils.DynamicCache, 'update', DynamicCache_update), \\\n",
    "        f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\"\n",
    "    assert update_attr(cache_utils.DynamicCache, 'get_seq_length', DynamicCache_get_seq_length), \\\n",
    "        f\"Unknown DynamicCache definition: {cache_utils.DynamicCache}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Instantiate adapted FP32 model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================Fixed setting that should not be changed by users==============\n",
    "# Auto-regression length: number of tokens to consume and number of logits to produce.\n",
    "# This value should NOT be changed due to downstream consumption requirements\n",
    "ARN = int(os.getenv(\"ARN\", 473))\n",
    "\n",
    "enable_right_padding = False   # right-pad causes error in model prepare step, only support left-pad currently\n",
    "pad_to_left = not enable_right_padding\n",
    "num_slices = None\n",
    "\n",
    "setattr(lmm_config.text_config, 'return_new_key_value_only', True)\n",
    "setattr(lmm_config.text_config, 'transposed_key_cache', True)\n",
    "setattr(lmm_config.text_config, 'use_combined_mask_input', True)\n",
    "setattr(lmm_config.text_config, 'use_position_embedding_input', True)\n",
    "setattr(lmm_config.text_config, '_attn_implementation', 'eager')\n",
    "setattr(lmm_config.text_config, '_attn_implementation_internal', 'eager')\n",
    "setattr(lmm_config.text_config, 'return_dict', False)\n",
    "setattr(lmm_config.text_config, 'logits_to_keep', 0)\n",
    "setattr(lmm_config.text_config, 'input_tokens_per_inference', ARN)\n",
    "\n",
    "lmm_config.save_pretrained(output_dir)\n",
    "\n",
    "with event_marker('Adapted FP model creation'):\n",
    "    model = modeling_gemma3.Gemma3ForConditionalGeneration.from_pretrained(model_id, config=lmm_config, cache_dir=cache_dir)\n",
    "    # Gemma3ForConditionalGeneration uses AutoModelForCausalLM to initialize language model, \n",
    "    # so we need below line to make sure we use the forward function of adapted Gemma3ForCausalLM\n",
    "    model.language_model.forward = types.MethodType(Gemma3ForCausalLM.forward, model.language_model)\n",
    "    \n",
    "    # Load and replace the original language model if an optimized language model is provided\n",
    "    if load_optimized_weights:\n",
    "        optimized_language_model = modeling_gemma3.Gemma3ForCausalLM.from_pretrained(optimized_model_id, config=lmm_config.text_config, cache_dir=cache_dir)\n",
    "        model.language_model = optimized_language_model\n",
    "        del optimized_language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Complete the last step(s) of Model Adaptation\n",
    "The following model adaptation are enabled for inference:\n",
    "- apply linear to conv in attention, MLP and lmhead and arrange linear weights properly for conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.common.dev.model_adaptation.linear_to_conv import replace_linears_with_convs\n",
    "\n",
    "with event_marker('FP model adaptation for NSP backend completion'):\n",
    "    model.language_model = replace_linears_with_convs(model.language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Instantiate VEG (vision tower + projector) and embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEmbeddingGenerator(torch.nn.Module):\n",
    "    def __init__(self, vision_tower, multi_modal_projector):\n",
    "        super().__init__()\n",
    "        self.multi_modal_projector = multi_modal_projector\n",
    "        self.vision_tower = vision_tower\n",
    "        self.device = vision_tower.device\n",
    "\n",
    "    # this forwrad gets the image pixel values that we get from the AutoProcessor when we pass the image and text (text -> input ids, and image-> pixel values)\n",
    "    # input shape is [1,3,896,896], output shape is [1,256,2560]\n",
    "    def forward(self, pixel_values):\n",
    "        image_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n",
    "        image_features = self.multi_modal_projector(image_outputs)\n",
    "        return image_features\n",
    "\n",
    "embedding_layer = copy.deepcopy(model.language_model.model.embed_tokens)\n",
    "vision_model = VisualEmbeddingGenerator(model.vision_tower, model.multi_modal_projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Changes to HuggingFace model to work with the Adapted Model and Prepared Model\n",
    "- As a result of adapting the model we introduce changes to the types of the model inputs.\n",
    "- As a result of model preparation, we make the shapes of the inputs static.\n",
    "- adapted_model_forward works with either adapted model dynamic input or prepared model static input model through flag static_shape.\n",
    "- Override the 'forward' function and the function 'prepare_inputs_for_generation'. With these overrides, we make the adapted model or prepared model work just like the original model.\n",
    "- adapted_model_prepare_inputs_for_dynamic_shapes is utility function for forward pass of adapted model with dynamic shapes.\n",
    "- adapted_model_prepare_inputs_for_static_shapes is utility function for forward pass of prepared model with static shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1 Define prepare inputs function for adapted model with dynamic shapes\n",
    "The inputs in case of dynamic shape is sent in the same way as we would have in the origial HF model but with the adaptation/ pre-computation logic. This means we do not need to perform any padding to our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.cache_utils import DynamicCache\n",
    "from genai_lib.llm.static_graph_utils import llm_create_1d_attn_mask, llm_pad_position_ids\n",
    "from gemma3.utils import llm_update_causal_mask, llm_create_position_embeddings, lmm_preprocess_inputs, llm_get_kv_length\n",
    "\n",
    "llm_config = lmm_config.text_config\n",
    "global_layer_idx = llm_config.sliding_window_pattern - 1\n",
    "\n",
    "# Creating a list to store which layers to sliding eviction\n",
    "layer_indices_to_perform_sliding_eviction = [layer_idx for layer_idx in range(llm_config.num_hidden_layers) if bool((layer_idx + 1) % llm_config.sliding_window_pattern)]\n",
    "\n",
    "\n",
    "def adapted_model_prepare_inputs_for_dynamic_shapes(self,inputs_embeds_slice, attn_mask_slice, position_ids_slice, outputs, token_type_ids=None, **kwargs):\n",
    "    device = inputs_embeds_slice.device\n",
    "    batch_size = inputs_embeds_slice.shape[0]\n",
    "    input_len = inputs_embeds_slice.shape[1]\n",
    "    kv_length, kv_length_sliding = llm_get_kv_length(outputs, global_layer_idx, layer_indices_to_perform_sliding_eviction)\n",
    "    \n",
    "    if outputs['past_key_values'] is None:\n",
    "        outputs['past_key_values'] = DynamicCache()\n",
    "\n",
    "    ########### Causal Mask preparation #######\n",
    "    # 1. Global Layer\n",
    "    past_kv_attn_mask = torch.ones((batch_size, kv_length), dtype=torch.long, device=device)\n",
    "\n",
    "    prepared_1d_attention_mask = llm_create_1d_attn_mask(attn_mask_past_kv=past_kv_attn_mask,\n",
    "                                                         attn_mask_input=attn_mask_slice)\n",
    "\n",
    "    # During dynamic mask creation, the KV$ update inside the Attention block will only perform the concat since our pastKV$ \n",
    "    # will not have space to scatter new KV$. As such do not pass pad_to_left = True or cache_index to the llm_update_causal_mask API.\n",
    "    prepared_causal_mask = llm_update_causal_mask(prepared_1d_attn_mask=prepared_1d_attention_mask,\n",
    "                                                  input_tensor=inputs_embeds_slice,\n",
    "                                                  max_input_tokens=input_len,\n",
    "                                                  model_context_len=kv_length+input_len,\n",
    "                                                  model_id_or_path=model_id,\n",
    "                                                  token_type_ids=token_type_ids)\n",
    "\n",
    "    # 2. Sliding Layer\n",
    "    past_kv_attn_mask_sliding = torch.ones((batch_size, kv_length_sliding), dtype=torch.long, device=device)\n",
    "\n",
    "    prepared_1d_attention_mask_sliding = llm_create_1d_attn_mask(attn_mask_past_kv=past_kv_attn_mask_sliding,\n",
    "                                                                 attn_mask_input=attn_mask_slice)\n",
    "\n",
    "    # Here we pass sliding_window to get the strided attention mask to ensure each token in the current input only looks at the most\n",
    "    # recent sliding window worth of KV$\n",
    "    swa_attention_mask  = llm_update_causal_mask(prepared_1d_attn_mask=prepared_1d_attention_mask_sliding,\n",
    "                                                 input_tensor=inputs_embeds_slice,\n",
    "                                                 max_input_tokens=input_len,\n",
    "                                                 model_context_len=kv_length_sliding+input_len,\n",
    "                                                 model_id_or_path=model_id,\n",
    "                                                 sliding_window=llm_config.sliding_window,\n",
    "                                                 token_type_ids=token_type_ids)\n",
    "    \n",
    "    ########### Position ID preparation #######\n",
    "\n",
    "    padded_position_ids = llm_pad_position_ids(position_ids_slice=position_ids_slice,\n",
    "                                               max_input_tokens=input_len,\n",
    "                                               pad_to_left=pad_to_left)\n",
    "\n",
    "    # Global RoPE\n",
    "    prepared_position_embeddings = llm_create_position_embeddings(config=llm_config,\n",
    "                                                                  position_ids=padded_position_ids)\n",
    "\n",
    "    # Local RoPE\n",
    "    config_local = copy.deepcopy(llm_config)\n",
    "    config_local.rope_theta = llm_config.rope_local_base_freq\n",
    "    config_local.rope_scaling = {\"rope_type\": \"default\"}\n",
    "    swa_position_embeddings = llm_create_position_embeddings(config=config_local,\n",
    "                                                            position_ids=padded_position_ids)\n",
    "\n",
    "    prepared_inputs = {\n",
    "        'attention_mask': prepared_causal_mask,\n",
    "        'position_ids': prepared_position_embeddings,\n",
    "        'past_key_values': copy.deepcopy(outputs['past_key_values']),\n",
    "        'inputs_embeds': inputs_embeds_slice,\n",
    "        'swa_attention_mask': swa_attention_mask,\n",
    "        'swa_position_ids': swa_position_embeddings,\n",
    "    }\n",
    "\n",
    "    return prepared_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2 Define prepare inputs function for adapted model with static shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.llm.static_graph_utils import llm_pad_inputs, llm_pad_past_kv, llm_pad_input_attn_mask, llm_create_kv_attn_mask, llm_get_dummy_kv\n",
    "from genai_lib.llm.dev.model_adaptation.common.utils import KEY_CONCAT_AXIS, VALUE_CONCAT_AXIS\n",
    "\n",
    "def adapted_model_prepare_inputs_for_static_shapes(self, inputs_embeds_slice, attn_mask_slice, position_ids_slice, outputs, token_type_ids=None, **kwargs):\n",
    "    batch_size = inputs_embeds_slice.shape[0]\n",
    "    pad_token = tokenizer.eos_token_id\n",
    "    device = inputs_embeds_slice.device\n",
    "    head_dim = llm_config.head_dim if hasattr(llm_config, 'head_dim') else llm_config.hidden_size // llm_config.num_attention_heads\n",
    "    kv_length, kv_length_sliding = llm_get_kv_length(outputs, global_layer_idx, layer_indices_to_perform_sliding_eviction)\n",
    "\n",
    "    ####### input id preparation #######\n",
    "    pad_input_embeds = llm_pad_inputs(pad_token=pad_token,\n",
    "                                      max_input_tokens=ARN,\n",
    "                                      inputs_embeds_slice=inputs_embeds_slice,\n",
    "                                      pad_to_left=pad_to_left)\n",
    "\n",
    "    ####### KV input preparation #######\n",
    "    global_dummy_kv = llm_get_dummy_kv(batch_size=batch_size,\n",
    "                                       num_key_value_heads=llm_config.num_key_value_heads,\n",
    "                                       head_dim=head_dim,\n",
    "                                       key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                       device=device,\n",
    "                                       cache_len=context_length-ARN if pad_to_left else context_length)\n",
    "    \n",
    "    sliding_dummy_kv = llm_get_dummy_kv(batch_size=batch_size,\n",
    "                                        num_key_value_heads=llm_config.num_key_value_heads,\n",
    "                                        head_dim=head_dim,\n",
    "                                        key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                        device=device,\n",
    "                                        cache_len=llm_config.sliding_window-ARN if pad_to_left else llm_config.sliding_window)\n",
    "    \n",
    "    dummy_kv = [sliding_dummy_kv if ((layer_idx + 1) % llm_config.sliding_window_pattern) != 0 else global_dummy_kv\n",
    "                for layer_idx in range(llm_config.num_hidden_layers)]\n",
    "\n",
    "    padded_past_kv_in = llm_pad_past_kv(dummy_past_kv=dummy_kv,\n",
    "                                        unpadded_past_kv=outputs['past_key_values'],\n",
    "                                        num_hidden_layers=llm_config.num_hidden_layers,\n",
    "                                        key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                        value_concat_axis=VALUE_CONCAT_AXIS,\n",
    "                                        pad_to_left=pad_to_left)\n",
    "    \n",
    "    cache_index = None\n",
    "    swa_cache_index = None\n",
    "    if enable_right_padding:\n",
    "        cache_index = torch.tensor([kv_length], dtype=torch.int64, device=device)\n",
    "        swa_cache_index = torch.tensor([kv_length_sliding], dtype=torch.int64, device=device)  \n",
    "    \n",
    "    ######### Attention mask Input preparation #######\n",
    "    inp_attn_mask = llm_pad_input_attn_mask(attn_mask_slice=attn_mask_slice,\n",
    "                                            max_input_tokens=ARN,\n",
    "                                            pad_to_left=pad_to_left)\n",
    "    \n",
    "    # 1. Global Layer\n",
    "    past_kv_attn_mask = llm_create_kv_attn_mask(unpadded_past_kv=outputs['past_key_values'],\n",
    "                                                model_context_len=context_length,\n",
    "                                                max_input_tokens=ARN,\n",
    "                                                batch_size=batch_size,\n",
    "                                                device=device,\n",
    "                                                pad_to_left=pad_to_left,\n",
    "                                                global_layer_idx=global_layer_idx)\n",
    "    \n",
    "    prepared_1d_attention_mask = llm_create_1d_attn_mask(attn_mask_past_kv=past_kv_attn_mask,\n",
    "                                                         attn_mask_input=inp_attn_mask,\n",
    "                                                         cache_index=cache_index)\n",
    "\n",
    "    prepared_causal_mask = llm_update_causal_mask(prepared_1d_attn_mask=prepared_1d_attention_mask,\n",
    "                                                  input_tensor=pad_input_embeds,\n",
    "                                                  max_input_tokens=ARN,\n",
    "                                                  model_context_len=context_length,\n",
    "                                                  model_id_or_path=model_id,\n",
    "                                                  cache_index=cache_index,\n",
    "                                                  pad_to_left=pad_to_left,\n",
    "                                                  token_type_ids=token_type_ids)\n",
    "\n",
    "    ########### Position ID preparation #######\n",
    "    padded_position_ids = llm_pad_position_ids(position_ids_slice=position_ids_slice,\n",
    "                                               max_input_tokens=ARN, \n",
    "                                               pad_to_left=pad_to_left)\n",
    "\n",
    "    # Global RoPE\n",
    "    prepared_position_embeddings = llm_create_position_embeddings(config=llm_config,\n",
    "                                                                  position_ids=padded_position_ids)\n",
    "\n",
    "    # Local RoPE\n",
    "    config_local = copy.deepcopy(llm_config)\n",
    "    config_local.rope_theta = llm_config.rope_local_base_freq\n",
    "    config_local.rope_scaling = {\"rope_type\": \"default\"}\n",
    "    swa_position_embeddings = llm_create_position_embeddings(config=config_local,\n",
    "                                                                  position_ids=padded_position_ids)\n",
    "\n",
    "    # Computing the sliding_cache_indices\n",
    "    if enable_right_padding:\n",
    "        offset = max(0, cache_index.item() + ARN - llm_config.sliding_window)\n",
    "        prefix_kv_length = getattr(llm_config, \"prefix_kv_length\", 0)\n",
    "        sliding_cache_indices_left = torch.arange(0, prefix_kv_length)\n",
    "        sliding_cache_indices_right = torch.arange(offset+prefix_kv_length, offset+llm_config.sliding_window)\n",
    "    else:\n",
    "        # for left padding, during quantization the prefix_kv_len should be 0.\n",
    "        prefix_kv_length = getattr(llm_config, \"prefix_kv_length\", 0)\n",
    "        sliding_cache_indices_left = torch.arange(context_length-kv_length-prefix_kv_length, context_length-kv_length)\n",
    "\n",
    "        # for the right indices, choose the sliding window length worth of positions, shifting by prefix kv if present.\n",
    "        sliding_cache_indices_right = torch.arange(context_length-llm_config.sliding_window+prefix_kv_length, context_length)\n",
    "    sliding_cache_indices = torch.cat([sliding_cache_indices_left, sliding_cache_indices_right], dim=-1).to(device)\n",
    "    \n",
    "    swa_attention_mask = prepared_causal_mask[..., sliding_cache_indices]\n",
    "    \n",
    "    prepared_inputs = {\n",
    "        'attention_mask': prepared_causal_mask,\n",
    "        'position_ids': prepared_position_embeddings,\n",
    "        'past_key_values': padded_past_kv_in,\n",
    "        'inputs_embeds': pad_input_embeds,\n",
    "    }\n",
    "\n",
    "    if enable_right_padding:\n",
    "        prepared_inputs.update({'cache_index': cache_index})\n",
    "        prepared_inputs.update({'swa_cache_index': swa_cache_index})\n",
    "    prepared_inputs.update({'swa_attention_mask': swa_attention_mask})\n",
    "    prepared_inputs.update({'swa_position_ids': swa_position_embeddings})\n",
    "\n",
    "    return prepared_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.3 Define forward function for adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from genai_lib.llm.static_graph_utils import llm_slice_inputs_for_inference, llm_trim_pad_logits\n",
    "from genai_lib.llm.dev.model_adaptation.common.utils import llm_update_kv_cache, trim_current_kv\n",
    "from genai_lib.llm.long_context_utils import llm_scatter_exceeded_kv_using_rotating_eviction, replenish_rotating_index_cache\n",
    "from gemma3.utils import slice_token_type_ids\n",
    "\n",
    "# Redefinition of the forward function to work with model I/O adaptations and static shapes of the tensors that the model consumes as input\n",
    "def adapted_model_forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    pixel_values=None,\n",
    "    attention_mask=None,\n",
    "    past_key_values=None,\n",
    "    inputs_embeds=None,\n",
    "    token_type_ids=None,\n",
    "    return_dict=False,\n",
    "    output_hidden_states=False,\n",
    "    **kwargs\n",
    "):\n",
    "    head_dim = llm_config.head_dim if hasattr(llm_config, 'head_dim') else llm_config.hidden_size // llm_config.num_attention_heads\n",
    "    static_shape = hasattr(self, 'num_logits_to_return')\n",
    "    num_slices = kwargs.get('num_slices', None)\n",
    "    embedding_layer = kwargs.get(\"embedding_layer\", None)\n",
    "    vision_model = kwargs.get(\"vision_model\", None)\n",
    "\n",
    "    # dictionary to store the running output which contains the logits and the useful past kv cache until that execution\n",
    "    outputs = {'past_key_values': past_key_values}\n",
    "\n",
    "    kv_length, _ = llm_get_kv_length(outputs, global_layer_idx, layer_indices_to_perform_sliding_eviction)\n",
    "    if kv_length == 0:\n",
    "        self.rotating_eviction_cache = None\n",
    "    \n",
    "    # generate text + (vision) embeddings\n",
    "    inputs_embeds = lmm_preprocess_inputs(input_ids=input_ids, pixel_values=pixel_values, inputs_embeds=inputs_embeds,\n",
    "                                          past_key_values=past_key_values, image_token_index=lmm_config.image_token_index,\n",
    "                                          embedding_layer=embedding_layer, vision_model=vision_model)\n",
    "    \n",
    "    # create the generator which slices input into chunks of AR (and pads if necessary)\n",
    "    slice_inputs_gen_obj = llm_slice_inputs_for_inference(max_input_tokens=ARN if static_shape else input_ids.shape[-1],\n",
    "                                                          model_context_len=context_length,\n",
    "                                                          inputs_embeds=inputs_embeds,\n",
    "                                                          past_seen_tokens=kv_length)\n",
    "    \n",
    "    # create token type id slices for bidirectional attention mask\n",
    "    token_type_ids_slices = None\n",
    "    if token_type_ids is not None and kv_length == 0:\n",
    "        token_type_ids_slices = slice_token_type_ids(token_type_ids, max_input_tokens=ARN if static_shape else input_ids.shape[-1])\n",
    "    \n",
    "    for i, inputs in enumerate(slice_inputs_gen_obj):\n",
    "        inputs_embeds_slice = inputs['inputs_embeds_slice']\n",
    "        attn_mask_slice = inputs['attn_mask_slice']\n",
    "        position_ids_slice = inputs['position_ids_slice']\n",
    "        token_type_ids_slice = token_type_ids_slices[i] if token_type_ids_slices is not None else None\n",
    "        \n",
    "        if num_slices is not None and i >= num_slices:\n",
    "            break\n",
    "        \n",
    "        if static_shape:\n",
    "            prepared_inputs = adapted_model_prepare_inputs_for_static_shapes(self,inputs_embeds_slice=inputs_embeds_slice,\n",
    "                                                                             attn_mask_slice=attn_mask_slice, \n",
    "                                                                             position_ids_slice=position_ids_slice,\n",
    "                                                                             outputs=outputs, token_type_ids=token_type_ids_slice)\n",
    "        else:\n",
    "            prepared_inputs = adapted_model_prepare_inputs_for_dynamic_shapes(self, inputs_embeds_slice=inputs_embeds_slice,\n",
    "                                                                              attn_mask_slice=attn_mask_slice, \n",
    "                                                                              position_ids_slice=position_ids_slice,\n",
    "                                                                              outputs=outputs, token_type_ids=token_type_ids_slice)\n",
    "        \n",
    "        cur_outputs = self.model(**prepared_inputs)\n",
    "        if not static_shape:\n",
    "            cur_outputs = (self.lm_head(cur_outputs[0]),) + cur_outputs[1:]\n",
    "\n",
    "        ############# KV$ management outside the self.model #####################\n",
    "        outputs['past_key_values'] = llm_update_kv_cache(unpadded_past_kv=outputs['past_key_values'],\n",
    "                                                         current_key_values=cur_outputs[-1],\n",
    "                                                         key_concat_axis=KEY_CONCAT_AXIS,\n",
    "                                                         value_concat_axis=VALUE_CONCAT_AXIS,\n",
    "                                                         inputs_embeds_slice=inputs_embeds_slice,\n",
    "                                                         pad_to_left=pad_to_left)\n",
    "        \n",
    "        if static_shape:\n",
    "            # Replenish the rotating eviction cache as needed\n",
    "            self.rotating_eviction_cache = replenish_rotating_index_cache(cache_length=llm_config.sliding_window - ARN,\n",
    "                                                                          num_kv_heads=llm_config.num_key_value_heads,\n",
    "                                                                          head_dim=head_dim,\n",
    "                                                                          rotating_eviction_cache=self.rotating_eviction_cache)\n",
    "\n",
    "            num_sliding_extra_kvs = outputs['past_key_values'][layer_indices_to_perform_sliding_eviction[0]][1].shape[2] - (llm_config.sliding_window - ARN)\n",
    "            if num_sliding_extra_kvs > 0:\n",
    "                # if exceeding KV$ for sliding layer, evict the oldest KV$ out\n",
    "                outputs['past_key_values'], self.rotating_eviction_cache = llm_scatter_exceeded_kv_using_rotating_eviction(self.rotating_eviction_cache,\n",
    "                                                                                                                           outputs['past_key_values'],\n",
    "                                                                                                                           num_sliding_extra_kvs,\n",
    "                                                                                                                           KEY_CONCAT_AXIS,\n",
    "                                                                                                                           VALUE_CONCAT_AXIS,\n",
    "                                                                                                                           layer_indices_to_perform_sliding_eviction)\n",
    "        else:\n",
    "            # assuming old KV$ is on the left side, we want to remove the older KV from the left, the size left after trimming should be sliding_window-1\n",
    "            input_tensor = torch.randn((1, llm_config.sliding_window-1))\n",
    "            outputs['past_key_values'] = trim_current_kv(outputs['past_key_values'], input_tensor, KEY_CONCAT_AXIS, VALUE_CONCAT_AXIS,\n",
    "                                                         layer_indices_to_perform_trimming=layer_indices_to_perform_sliding_eviction)\n",
    "        \n",
    "        ############# Logit management outside the self.model #####################\n",
    "\n",
    "        lm_logits = llm_trim_pad_logits(cur_logits=cur_outputs[0],\n",
    "                                        inputs_embeds_slice=inputs_embeds_slice,\n",
    "                                        pad_to_left=pad_to_left)\n",
    "        bsz, _, dim = lm_logits.shape\n",
    "        outputs['logits'] = torch.cat(\n",
    "                (outputs.get('logits', torch.zeros((bsz, 0, dim), device=lm_logits.device)), lm_logits),\n",
    "                dim=1)\n",
    "\n",
    "    if return_dict:\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=outputs.get('loss', None),\n",
    "            logits=outputs.get('logits', None),\n",
    "            past_key_values=outputs.get('past_key_values', None),\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )\n",
    "    return outputs['logits'], outputs['past_key_values']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 Adapted FP model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "if run_ppl_eval:\n",
    "    model.language_model.forward = types.MethodType(adapted_model_forward, model.language_model)\n",
    "    with event_marker(f\"Adapted FP model eval\", flush_ram=True):\n",
    "        with place_model(model.language_model, torch.device('cuda')), place_model(vision_model, torch.device(\"cuda\")), place_model(embedding_layer, torch.device(\"cuda\")):\n",
    "            adapted_ppl = llm_evaluate_ppl_with_dataloader(model=model.language_model, dataloader=test_dataloader,\n",
    "                                                           model_forward_kwargs={\"embedding_layer\": embedding_layer, \"vision_model\": vision_model})\n",
    "        llm_lib_log_metric(ModelType.adapted_model, Metric.ppl, adapted_ppl, model_name=\"base\")\n",
    "        print(f\"PPL score of Adapted HF FP model = {adapted_ppl}\")\n",
    "\n",
    "    # Revert forward passes for model preparation\n",
    "    model.language_model.forward = types.MethodType(Gemma3ForCausalLM.forward, model.language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.utils import change_tensor_device_placement\n",
    "\n",
    "def get_dummy_data(device=\"cuda\", dtype=torch.float32, return_dict=False):\n",
    "    inputs_embeds = torch.rand(1, ARN, llm_config.hidden_size, device=device, dtype=dtype)\n",
    "    attn_mask = torch.ones((1, ARN), device=device, dtype=dtype)\n",
    "    position_ids = torch.cumsum(attn_mask, dim=1) - 1\n",
    "    outputs = {'past_key_values': None}\n",
    "    dummy_input = adapted_model_prepare_inputs_for_static_shapes(model.language_model, inputs_embeds, attn_mask, position_ids, outputs)\n",
    "    for val in dummy_input:\n",
    "        dummy_input[val] = change_tensor_device_placement(dummy_input[val], device)\n",
    "    if not return_dict:\n",
    "        dummy_input = tuple(dummy_input.values())\n",
    "\n",
    "    return dummy_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare model using QAIRT model preparer pro\n",
    "\n",
    "#### 5.1 KVCache MHA model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from qti.aisw.preparer_api import prepare_model\n",
    "from qti.aisw.emitter.utils.torch_utils import load_torch_model_using_safetensors\n",
    "\n",
    "from genai_lib.llm.model_preparation_utils import llm_build_preparer_converter_args\n",
    "from genai_lib.llm.utils import llm_model_input_output_names\n",
    "\n",
    "model.language_model.num_logits_to_return = ARN # configuring the model for KVCache mode\n",
    "\n",
    "skip_prepare = False  # whether to skip model prepare and use existing prepared model\n",
    "if skip_prepare:\n",
    "    prepare_path = \"<path to existing prepared model folder when skip_prepare=True>\"\n",
    "else:\n",
    "    prepare_path = os.path.join(output_dir, 'prepare')\n",
    "os.makedirs(prepare_path, exist_ok=True)\n",
    "prepare_filename = f'{model_name}_kvcache_{llm_config.num_hidden_layers}_layer'\n",
    "\n",
    "if skip_prepare:\n",
    "    with event_marker(f\"KVCache load pre-prepared {prepare_filename}\", flush_ram=True):\n",
    "        prepared_model_path = os.path.join(prepare_path, f'{prepare_filename}.py')\n",
    "        if not os.path.exists(prepared_model_path):\n",
    "            raise ValueError(f\"prepared artifacts not found in {prepare_path}\")\n",
    "        else:\n",
    "            print(f'WARNING: preparation skipped for model={prepare_filename}, prepared at {time.ctime(os.path.getmtime(prepared_model_path))}')\n",
    "            prepared_model = load_torch_model_using_safetensors(path=prepare_path, filename=prepare_filename, model_name=prepare_filename)\n",
    "\n",
    "else:\n",
    "    dummy_input = get_dummy_data(device=model.language_model.model.device, dtype=model.language_model.dtype, return_dict=True)\n",
    "    input_names, output_names = llm_model_input_output_names(llm_config.num_hidden_layers, use_input_embedding = True)\n",
    "    if enable_right_padding:\n",
    "        input_names += [\"cache_index\"]\n",
    "        input_names += [\"swa_cache_index\"]\n",
    "    input_names += ['swa_attention_mask']\n",
    "    input_names += ['swa_position_ids']  # sliding window attention RoPE embeddings\n",
    "    # Build converter args\n",
    "    # TODO: temporary fix - SWA RoPE embeddings (swa_position_ids) manually added to llm_build_preparer_converter_args in genai-lib (get the fix from the new commits)\n",
    "    converter_args = llm_build_preparer_converter_args(llm_config.num_hidden_layers, input_names, use_qairt_mpp=True)\n",
    "    with event_marker(\"KVCache prepare model\", flush_ram=True):\n",
    "        if __name__ == '__main__': # We use the main guard to prevent child processes from re-running the top-level code\n",
    "            prepared_model = prepare_model(model.language_model,\n",
    "                                          dummy_input,\n",
    "                                          model_name=prepare_filename,\n",
    "                                          filename=prepare_filename,\n",
    "                                          path=prepare_path,\n",
    "                                          input_names=input_names,\n",
    "                                          output_names=output_names,\n",
    "                                          onnx_export_args={\"opset_version\":17},\n",
    "                                          keep_original_model_structure=False, # Flatten the model to enable weight-sharing by setting `keep_original_model_structure = False\\n\",\n",
    "                                          converter_args=converter_args,\n",
    "                                          order_inputs=True,\n",
    "                                          order_outputs=True,\n",
    "                                          skipped_optimizers=['eliminate_common_subexpression',\n",
    "                                                              'eliminate_nop_with_unit', \n",
    "                                                              'eliminate_duplicate_initializer'\n",
    "                                                             ],\n",
    "                                           return_prepare_model=True,\n",
    "                                          )\n",
    "        else:\n",
    "            raise Exception(\"Killing multiprocessing spawn started by Converter during model preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation of prepared models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Changes to HuggingFace model to work with the prepared model\n",
    "Replace the model inside the HuggingFace model with the prepared model.\n",
    "Note that the prepared model already fuses model.model and model.lm_head \n",
    "into one, so here we simply set model.lm_head to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.language_model.model\n",
    "del model.language_model.lm_head\n",
    "\n",
    "model.language_model.model = prepared_model\n",
    "model.lm_head = None\n",
    "\n",
    "model.language_model.forward = types.MethodType(adapted_model_forward, model.language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Evaluation of perplexity score using prepared model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    with event_marker(\"KVcache prepared FP eval\", flush_ram=True):\n",
    "        with place_model(prepared_model, torch.device(\"cuda\")), place_model(vision_model, torch.device(\"cuda\")), place_model(embedding_layer, torch.device(\"cuda\")):\n",
    "            model.language_model.model = prepared_model\n",
    "            prepared_kvcache_ppl = llm_evaluate_ppl_with_dataloader(model=model.language_model, dataloader=test_dataloader,\n",
    "                                                                    model_forward_kwargs={\"embedding_layer\": embedding_layer, \"vision_model\": vision_model})\n",
    "        llm_lib_log_metric(ModelType.prepared_model, Metric.ppl, prepared_kvcache_ppl, model_name=\"base\")\n",
    "        print(f\"ppl score of KVCACHE prepared fp model = {prepared_kvcache_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Quantization\n",
    "\n",
    "The _Quantization_ step is the primary focus of this notebook, this section could be modified to execute various quantization experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for quantization\n",
    "apply_lm_head_seqmse = False\n",
    "apply_decoder_seqmse = True\n",
    "apply_lm_head_lpbq = False\n",
    "apply_decoder_lpbq = False\n",
    "num_seqmse_candidates = 60\n",
    "num_seqmse_batches = 20\n",
    "num_calibration_batches = 20\n",
    "embedding_table_bitwidth = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Create quantsim configured for QNN HTP target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from copy import deepcopy\n",
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v2.quantsim import QuantizationSimModel\n",
    "\n",
    "if apply_lm_head_seqmse or apply_decoder_seqmse:\n",
    "    import functools\n",
    "\n",
    "    def copy_model_with_shared_weights(source_model):\n",
    "        target_model = deepcopy(source_model)\n",
    "        for name, source_parameter in source_model.named_parameters():\n",
    "            pre, _, post = name.rpartition('.')\n",
    "            pre_obj = functools.reduce(getattr, [target_model] + pre.split('.')) if pre else target_model\n",
    "            setattr(pre_obj, post, source_parameter)\n",
    "        return target_model\n",
    "\n",
    "    # Create copy of fp model defintion for SeqMSE and/or LoRA\n",
    "    fp_prepared_model = copy_model_with_shared_weights(prepared_model)\n",
    "\n",
    "dummy_input = get_dummy_data(device=\"cuda\", dtype=model.language_model.dtype, return_dict=True)\n",
    "\n",
    "dummy_input_sorted = {}\n",
    "sig = inspect.signature(prepared_model.forward)\n",
    "for key in list(sig.parameters.keys()):\n",
    "    dummy_input_sorted[key] = dummy_input[key]\n",
    "dummy_input = tuple(dummy_input_sorted.values())\n",
    "\n",
    "with event_marker(\"create KVCache Quantsim\"):\n",
    "    with place_model(prepared_model, \"cuda\"):\n",
    "        quantsim = QuantizationSimModel(model=prepared_model,\n",
    "                                        quant_scheme=QuantScheme.post_training_tf,\n",
    "                                        dummy_input=dummy_input,\n",
    "                                        default_output_bw=16,\n",
    "                                        default_param_bw=4,\n",
    "                                        in_place=True,\n",
    "                                        config_file=htp_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Setting 16bit x 8bit matmuls\n",
    "To keep key and value tensors as 8 bits, reducing data I/O costs associated with KV-cache orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.experimental.quantsim_utils import set_matmul_second_input_producer_to_8bit_symmetric\n",
    "\n",
    "set_matmul_second_input_producer_to_8bit_symmetric(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Concat encoding unification\n",
    "configuring concat ops to have shared encoding on input and output activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.experimental import propagate_output_encodings\n",
    "from aimet_torch.nn.modules import custom as aimet_ops\n",
    "\n",
    "propagate_output_encodings(quantsim, aimet_ops.Concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Manual Mixed Precision\n",
    "applying mixed precision configuration to ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llm_utils.mixed_precision_overrides import ManualQuantsimMixedPrecisionConfig\n",
    "from aimet_torch.v2.nn.modules.custom import QuantizedRmsNorm\n",
    "from aimet_torch.v2.quantization.affine import QuantizeDequantize\n",
    "\n",
    "def apply_manual_mixed_precision(sim):\n",
    "    with open(\"./config/mixed_precision_config/exceptions.json\", \"r\") as f_in:\n",
    "        mixed_precision_config = json.load(f_in)\n",
    "\n",
    "    # Customize mixed precision llm_config based on user parameters\n",
    "    for entry in mixed_precision_config['name_list']:\n",
    "        if \"model_embed_tokens_Gather\" in entry['module_name']:\n",
    "            entry['exceptions']['param_exceptions']['bitwidth'] = embedding_table_bitwidth\n",
    "            break\n",
    "\n",
    "    quantsim_adjuster = ManualQuantsimMixedPrecisionConfig(mixed_precision_config_file=mixed_precision_config)\n",
    "    quantsim_adjuster.apply_exceptions(sim)\n",
    "\n",
    "    # Make RMSNorm encodings per-tensor (they default to per-channel)\n",
    "    for name, qmodule in sim.named_qmodules():\n",
    "        if isinstance(qmodule, QuantizedRmsNorm):\n",
    "            qmodule.param_quantizers['weight'] = QuantizeDequantize(shape=(), bitwidth=16, symmetric=False).to(qmodule.weight.device)\n",
    "\n",
    "apply_manual_mixed_precision(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Apply Block Quantization\n",
    "\n",
    "Swapping needed modules' weight quantizers to LPBQ quantizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.v2.nn.true_quant import QuantizedConv2d\n",
    "from aimet_torch.v2.quantsim.config_utils import set_grouped_blockwise_quantization_for_weights\n",
    "\n",
    "def apply_lpbq(sim):\n",
    "    arg = None\n",
    "\n",
    "    if apply_decoder_lpbq and apply_lm_head_lpbq:\n",
    "        arg = lambda module: isinstance(module, QuantizedConv2d)\n",
    "    elif apply_decoder_lpbq:\n",
    "        arg = lambda module: isinstance(module, QuantizedConv2d) and module.param_quantizers['weight'].bitwidth == 4\n",
    "    elif apply_lm_head_lpbq:\n",
    "        lm_head_modules = [qmodule for name, qmodule in sim.named_qmodules() if \"lm_head\" in name]\n",
    "        arg = lambda module: module in lm_head_modules and isinstance(module, QuantizedConv2d)\n",
    "\n",
    "    if arg:\n",
    "        BLOCK_QUANT_SIZE = 64\n",
    "        set_grouped_blockwise_quantization_for_weights(sim=sim,\n",
    "                                                       arg=arg,\n",
    "                                                       bitwidth=4,\n",
    "                                                       symmetric=True,\n",
    "                                                       decompressed_bw=8,\n",
    "                                                       block_size=BLOCK_QUANT_SIZE,\n",
    "                                                       block_grouping=-1)\n",
    "\n",
    "\n",
    "if apply_decoder_lpbq or apply_lm_head_lpbq:\n",
    "    apply_lpbq(quantsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 7.7 Sequential MSE\n",
    "applying sequential MSE technique to optimize parameter encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from aimet_torch.v2.seq_mse import apply_seq_mse, SeqMseParams\n",
    "\n",
    "def perform_seqmse(sim, fp_model):\n",
    "    def _seq_mse_forward_fn(_model, inputs):\n",
    "        model.language_model.model = _model\n",
    "        inputs.update({\"embedding_layer\": embedding_layer, \"vision_model\": vision_model})\n",
    "        model.language_model(**inputs)\n",
    "\n",
    "    lm_head_fp_modules = [module\n",
    "                          for module_name, module in fp_model.named_modules()\n",
    "                          if isinstance(module, torch.nn.Conv2d) and 'lm_head' in module_name]\n",
    "    decoder_fp_modules = [module\n",
    "                          for module_name, module in fp_model.named_modules()\n",
    "                          if isinstance(module, torch.nn.Conv2d) and 'lm_head' not in module_name]\n",
    "\n",
    "    if apply_decoder_seqmse and apply_lm_head_seqmse:\n",
    "        modules_to_exclude = []\n",
    "    elif apply_decoder_seqmse:\n",
    "        modules_to_exclude = lm_head_fp_modules\n",
    "    elif apply_lm_head_seqmse:\n",
    "        modules_to_exclude = decoder_fp_modules\n",
    "    \n",
    "    seqmse_params = SeqMseParams(num_batches=num_seqmse_batches,\n",
    "                                 inp_symmetry='symqt',\n",
    "                                 num_candidates=num_seqmse_candidates,\n",
    "                                 loss_fn='mse',\n",
    "                                 forward_fn=_seq_mse_forward_fn)\n",
    "\n",
    "    with place_model(sim.model, torch.device(\"cuda\")), place_model(fp_model, torch.device(\"cuda\")), \\\n",
    "         place_model(embedding_layer, torch.device(\"cuda\")), place_model(vision_model, torch.device(\"cuda\")):\n",
    "        with torch.no_grad():\n",
    "            apply_seq_mse(fp_model, sim, llava_dataset, seqmse_params, modules_to_exclude=modules_to_exclude)\n",
    "            # apply_seq_mse(fp_model, sim, train_dataloader, seqmse_params, modules_to_exclude=modules_to_exclude)\n",
    "\n",
    "\n",
    "if apply_decoder_seqmse or apply_lm_head_seqmse:\n",
    "    with event_marker(\"SeqMSE for base model\"):\n",
    "        perform_seqmse(quantsim, fp_prepared_model)\n",
    "\n",
    "    del fp_prepared_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.8 Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from aimet_torch.v2.experimental.quantsim_utils import clip_weights_to_7f7f\n",
    "\n",
    "def perform_calibration(sim, calibration_dataloader, num_batches=200):\n",
    "    def _calibration_forward_fn(sim_model, kwargs):\n",
    "        model.language_model.model = sim_model\n",
    "        data_loader = kwargs['data_loader']\n",
    "        max_iterations = kwargs['num_batches']\n",
    "        for batch_id, batch in enumerate(tqdm(data_loader, total=max_iterations)):\n",
    "            if batch_id < max_iterations:\n",
    "                inputs = change_tensor_device_placement(batch, device=\"cuda\")\n",
    "                inputs.update({\"embedding_layer\": embedding_layer, \"vision_model\": vision_model})\n",
    "                _ = model.language_model(**inputs)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    kwargs = {\n",
    "        'data_loader': calibration_dataloader,\n",
    "        'num_batches': num_batches\n",
    "    }\n",
    "\n",
    "    with place_model(sim.model, \"cuda\"), place_model(embedding_layer, torch.device(\"cuda\")), place_model(vision_model, torch.device(\"cuda\")):\n",
    "        with torch.no_grad():\n",
    "            sim.compute_encodings(_calibration_forward_fn, kwargs)\n",
    "\n",
    "    clip_weights_to_7f7f(sim)\n",
    "\n",
    "\n",
    "with event_marker(\"compute encoding for base model\", flush_ram=True):\n",
    "    perform_calibration(quantsim, llava_dataset, num_batches=num_calibration_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9 Eval KV Cache quantsim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    with event_marker(\"KV cache quantsim model eval\", flush_ram=True):\n",
    "        with place_model(quantsim.model, torch.device(\"cuda\")), place_model(vision_model, torch.device(\"cuda\")), place_model(embedding_layer, torch.device(\"cuda\")):\n",
    "            model.language_model.model = quantsim.model\n",
    "            sim_ppl = llm_evaluate_ppl_with_dataloader(model=model.language_model, dataloader=test_dataloader,\n",
    "                                                       model_forward_kwargs={\"embedding_layer\": embedding_layer, \"vision_model\": vision_model})\n",
    "        llm_lib_log_metric(ModelType.qsim_model, Metric.ppl, sim_ppl, model_name=\"base\")\n",
    "        print(f\"ppl score of KVCACHE quantsim model = {sim_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Export\n",
    "the pipeline call below would export onnx model, encoding and test vector for KVCache models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Export Onnx and Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch import onnx_utils\n",
    "from aimet_torch.onnx_utils import OnnxExportApiArgs\n",
    "\n",
    "def export_onnx_and_encodings(sim, onnx_dir, filename_prefix, generate_updatable_tensors = False):\n",
    "    # Get input names and output names. This is different from the input names and output names we created for model preparation. \n",
    "    # The reason for this difference stems from the fact that we want the prepared model to have inputs and outputs named similar to original HF model\n",
    "    # ONNX does not allow tupling the inputs or outputs and we want to give meaningful names to the input and output tensors in the ONNX graph\n",
    "    input_names, output_names = llm_model_input_output_names(llm_config.num_hidden_layers,\n",
    "                                                             use_position_embedding_input=True,\n",
    "                                                             separate_tuple_input_output=True,\n",
    "                                                             use_input_embedding=True)\n",
    "    if enable_right_padding:\n",
    "        input_names += [\"cache_index\"]\n",
    "        input_names += [\"swa_cache_index\"]\n",
    "    input_names += [\"swa_attention_mask\"]\n",
    "    input_names += ['swa_position_ids_cos', 'swa_position_ids_sin']  # sliding window attention RoPE embeddings\n",
    "\n",
    "    # Replace past_ with swa_ in the input and output names of swa layers\n",
    "    for index, name in enumerate(input_names + output_names):\n",
    "        if name.startswith(\"past_key\") or name.startswith(\"past_value\"):\n",
    "            kv_index = int([i for i in name.split(\"_\") if i.isdigit()][0])\n",
    "            if ((kv_index + 1) % llm_config.sliding_window_pattern) != 0:\n",
    "                if index < len(input_names):\n",
    "                    input_names[index] = name.replace(f\"past_\", f\"swa_\")\n",
    "                else:\n",
    "                    output_names[index - len(input_names)] = name.replace(f\"past_\", f\"swa_\")\n",
    "\n",
    "    # Setting this flag to False means that the prepared model will be flattened\n",
    "    onnx_utils.EXPORT_TO_ONNX_DIRECT = True\n",
    "    \n",
    "    onnx_api_args = OnnxExportApiArgs(input_names=input_names, output_names=output_names, opset_version=17)\n",
    "    onnx_utils.RESTORE_ONNX_MODEL_INITIALIZERS = True\n",
    "\n",
    "    dummy_input = get_dummy_data(device=\"cpu\", dtype=model.language_model.dtype, return_dict=True)\n",
    "    dummy_input_sorted = {}\n",
    "    sig = inspect.signature(sim.model.forward)\n",
    "    for key in list(sig.parameters.keys()):\n",
    "        dummy_input_sorted[key] = dummy_input[key]\n",
    "    dummy_input = tuple(dummy_input_sorted.values())\n",
    "\n",
    "    os.makedirs(onnx_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        with place_model(sim.model, torch.device(\"cpu\")):\n",
    "            sim.export(onnx_dir, filename_prefix, dummy_input, onnx_export_args=onnx_api_args,\n",
    "                       export_model=True, filename_prefix_encodings=filename_prefix)\n",
    "\n",
    "\n",
    "with event_marker(f\"KVCache export onnx and encodings for base generation model\", flush_ram=True):\n",
    "    base_onnx_dir = os.path.join(output_dir, 'export', 'onnx')\n",
    "    base_filename_prefix = f\"{model_name}\"\n",
    "    export_onnx_and_encodings(quantsim, base_onnx_dir, base_filename_prefix)\n",
    "\n",
    "# Exporting Tokenizer\n",
    "tokenizer_dir = os.path.join(output_dir, 'tokenizer')\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "# Export embedding bin\n",
    "inputs_embeds = embedding_layer.weight * embedding_layer.embed_scale\n",
    "lut_array= inputs_embeds.detach().cpu().numpy()\n",
    "lut_array.tofile(os.path.join(output_dir,\"embedding_fp32.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Generating test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from genai_lib.llm.test_vectors import generate_test_vectors\n",
    "\n",
    "def generate_test_vectors_for_usecase(sim, output_dir, num_test_vectors = 1, slice_num = 0):\n",
    "    test_vector_layers = [\n",
    "        \"model_embed_tokens_Gather\",\n",
    "        \"model_layers_\\\\d+_Add_1\"\n",
    "    ]\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    with place_model(sim.model, device), place_model(vision_model, torch.device(\"cuda\")), place_model(embedding_layer, torch.device(\"cuda\")):\n",
    "        for index, batch in enumerate(train_dataloader):\n",
    "            if index >= num_test_vectors:\n",
    "                break\n",
    "            \n",
    "            # Consider LLM data (wikitext) here\n",
    "            inputs_embeds_slice = embedding_layer(batch['input_ids'][..., :ARN].to(device=torch.device('cuda')))\n",
    "            attention_mask_slice = torch.ones((inputs_embeds_slice.shape[0], ARN), dtype = torch.long, device=torch.device('cuda'))\n",
    "            position_ids_slice = (torch.cumsum(attention_mask_slice, dim=1) - 1).to(device=torch.device('cuda'))\n",
    "            outputs = {'past_key_values': None}\n",
    "            model_inputs = adapted_model_prepare_inputs_for_static_shapes(model.language_model, \n",
    "                                                                          inputs_embeds_slice=inputs_embeds_slice, \n",
    "                                                                          attn_mask_slice=attention_mask_slice, \n",
    "                                                                          position_ids_slice=position_ids_slice,\n",
    "                                                                          outputs=outputs)\n",
    "            \n",
    "            generate_test_vectors(sim=sim, model_inputs=model_inputs, output_dir=output_dir,\n",
    "                                  batch_index=index, test_vector_layers=test_vector_layers)\n",
    "\n",
    "with event_marker(\"generate base model test vectors\"):\n",
    "    test_vec_dir = os.path.join(output_dir, 'export', 'test_vectors')\n",
    "    generate_test_vectors_for_usecase(quantsim, os.path.dirname(test_vec_dir),\n",
    "                                      slice_num=1 + (llm_config.sliding_window + ARN - 1) // ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Save Quantsim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Increase recursion depth limit to save full model\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "# base_dir = os.path.join(output_dir, 'quantsim')\n",
    "with event_marker(\"save quantsim model\"), open(f\"{output_dir}/{prepare_filename}.pkl\", 'wb') as file:\n",
    "    pkl.dump(quantsim, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from genai_lib.common.debug.profiler import EventProfiler\n",
    "from genai_lib.common.debug.recipe_logger import dump_logs_to_json\n",
    "\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(output_dir, 'profiling_stats.json'))\n",
    "dump_logs_to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
