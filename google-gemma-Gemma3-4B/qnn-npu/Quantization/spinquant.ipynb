{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpinQuant-R2 Optimization for Gemma3-4B language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Required packages\n",
    "The notebook assumes AIMET and Gemma3 related packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages only if running in jupyter notebook mode\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    !sudo -H apt-get -qq update\n",
    "    !sudo -H apt-get -qq install libc++-dev\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir transformers==4.50.0\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir tokenizers==0.21.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow\n",
    "This notebook covers the following\n",
    "1. Instantiate model and dataloaders\n",
    "2. Apply SpinQuant-R2\n",
    "3. Save optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instantiate model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from genai_lib.common.debug.recipe_logger import recipe_dump_init\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_env_info\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoProcessor\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "\n",
    "#======================Configurable setting by users================================\n",
    "context_length = 8192\n",
    "run_ppl_eval = True\n",
    "run_spinquant_r2 = True\n",
    "\n",
    "cache_dir='/tmp/cache_dir'\n",
    "output_dir = '/tmp/output_dir'  # point to where the export artifacts of this notebook to be saved\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# HF configs\n",
    "model_name = 'gemma_4b'\n",
    "model_id=\"google/gemma-3-4b-it\"  # HF checkpoint\n",
    "\n",
    "llm_config = AutoConfig.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# Recipe_logger: Initialize the logger and log environment details \n",
    "recipe_dump_init(output_dir)\n",
    "llm_lib_log_env_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Instantiate the HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.gemma3 import modeling_gemma3\n",
    "from genai_lib.common.debug.profiler import event_marker\n",
    "\n",
    "with event_marker('Load FP model'):\n",
    "    model = modeling_gemma3.Gemma3ForConditionalGeneration.from_pretrained(model_id, config=llm_config, cache_dir=cache_dir)\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = '0'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, use_fast=True, trust_remote_code=True)\n",
    "    processor = AutoProcessor.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "    ## Adjust the tokenizer to limit to context_length\n",
    "    tokenizer.model_max_length = context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Instantiate Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.wikitext_dataloader import get_wiki_dataset\n",
    "\n",
    "# Wikitext dataloader\n",
    "train_dataloader, test_dataloader, _ = get_wiki_dataset(context_length, tokenizer, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 HuggingFace FP model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.utils import place_model\n",
    "from genai_lib.llm.evaluation_utils import llm_evaluate_ppl_with_dataloader\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_property, Property\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_metric, ModelType, Metric\n",
    "\n",
    "\n",
    "# Recipe_logger: Log the context_length property and the metrics.\n",
    "llm_lib_log_property({Property.context_length : context_length})\n",
    "\n",
    "if run_ppl_eval:\n",
    "    with event_marker(\"HuggingFace FP model eval\"):\n",
    "        with place_model(model, torch.device('cuda')):\n",
    "            orig_ppl = llm_evaluate_ppl_with_dataloader(model=model.language_model, dataloader=test_dataloader)\n",
    "\n",
    "    llm_lib_log_metric(ModelType.hf_model, Metric.ppl, orig_ppl, model_name=\"base\")\n",
    "    print(f\"PPL score of HuggingFace FP model: {orig_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply SpinQuant-R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Apply Rotation\n",
    "\n",
    "Apply SpinQuant-R2 to v/o-proj weights (the weights are updated in place). This is to make the V-proj activations more quantization friendly since we are using 8-bits KV-cache on target and this causes large accuracy drop on this model. We do not apply R1 here since Gemma3 has post-RMSNorm for both attention and MLP blocks, which makes R1 non-mergable (i.e., can only be online rotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.spinquant.spinquant_utils import apply_spinquant_r2\n",
    "\n",
    "if run_spinquant_r2:\n",
    "    with event_marker('Apply SpinQuant R2'):\n",
    "        with place_model(model, torch.device('cuda')):\n",
    "            apply_spinquant_r2(model=model.language_model, config=llm_config.text_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Evaluate SpinQuant FP Model\n",
    "\n",
    "The SpinQuant model (i.e., with R2 merged to v/o-proj) should give the same FP PPL as the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_ppl_eval:\n",
    "    with event_marker(\"SpinQuant FP model eval\"):\n",
    "        with place_model(model, torch.device('cuda')):\n",
    "            spin_ppl = llm_evaluate_ppl_with_dataloader(model=model.language_model, dataloader=test_dataloader)\n",
    "    print(f\"PPL score of SpinQuant FP model: {spin_ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save optimized model\n",
    "\n",
    "This notebook exports new model weights that are more quantization friendly, and stores them in the specified spinquant directory. To use the new weights for the quantization pipeline notebook (`gemma3_4b.ipynb`), pass in the spinquant directory path as the Model ID in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinquant_dir = os.path.join(output_dir, 'spinquant')\n",
    "os.makedirs(spinquant_dir, exist_ok=True)\n",
    "\n",
    "with event_marker(\"save optimized model\", flush_ram=True):\n",
    "    model.language_model.save_pretrained(spinquant_dir)\n",
    "    tokenizer.save_pretrained(spinquant_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from genai_lib.common.debug.profiler import EventProfiler\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(output_dir, 'profiling_stats.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
