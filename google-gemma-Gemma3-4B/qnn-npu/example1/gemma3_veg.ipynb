{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMET Quantization workflow for Gemma3-4B Vision Encoder\n",
    "\n",
    "This notebook shows a working code example of how to use AIMET to quantize Gemma3-4B Vision Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Required packages\n",
    "The notebook assumes AIMET and Gemma3 related packages are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != '__main__':\n",
    "    raise Exception(\"Killing multiprocessing spawn started by Converter during model preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages only if running in jupyter notebook mode\n",
    "if hasattr(__builtins__,'__IPYTHON__'):\n",
    "    !sudo -H apt-get -qq update\n",
    "    !sudo -H apt-get -qq install libc++-dev\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir transformers==4.50.0\n",
    "    !sudo -H pip install --quiet --upgrade --root-user-action=ignore --no-cache-dir tokenizers==0.21.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall flow\n",
    "This notebook covers the following\n",
    "1. Setting QNN SDK and NSP target\n",
    "2. Instantiate HuggingFace model and Dataloader\n",
    "3. Model adaptation\n",
    "4. Prepare model using QAIRT model preparer pro\n",
    "5. Quantization \n",
    "6. Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setting QNN SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "\n",
    "QNN_SDK_ROOT = \"/tmp/qnn\"\n",
    "assert QNN_SDK_ROOT is not None, 'Please point the QNN_SDK_ROOT variable to your QNN SDK'\n",
    "assert os.path.exists(QNN_SDK_ROOT), \"QNN_SDK_ROOT doesn't exist!\"\n",
    "sys.path.insert(0, QNN_SDK_ROOT + '/lib/python')\n",
    "\n",
    "lib_clang_path = os.path.join(QNN_SDK_ROOT, 'lib', 'x86_64-linux-clang')\n",
    "LD_LIBRARY_PATH = os.getenv('LD_LIBRARY_PATH', None)\n",
    "os.environ['LD_LIBRARY_PATH'] = lib_clang_path + ':' + LD_LIBRARY_PATH if LD_LIBRARY_PATH is not None else lib_clang_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setting NSP Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./')\n",
    "# Select quantsim config based on target\n",
    "htp_config_file = f'htp_quantsim_config_v81.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate HuggingFace model and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from genai_lib.common.debug.recipe_logger import recipe_dump_init\n",
    "from genai_lib.common.debug.recipe_logger import llm_lib_log_env_info\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoProcessor, AutoImageProcessor\n",
    "\n",
    "#======================Configurable setting by users================================\n",
    "run_sqnr_eval = True\n",
    "\n",
    "model_name = 'siglip'\n",
    "model_id=\"google/gemma-3-4b-it\"\n",
    "\n",
    "cache_dir='/tmp/cache_dir'\n",
    "output_dir = '/tmp/output_dir'  # point to where the export artifacts of this notebook to be saved\n",
    "\n",
    "llm_config = AutoConfig.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "# To help with debugging num_hidden_layers could be set to 2 to quickly verify the pipeline and export a two layer model for verification purposes\n",
    "num_hidden_layers = int(os.getenv(\"NUM_HIDDEN_LAYERS\", 0))\n",
    "llm_config.vision_config.num_hidden_layers = num_hidden_layers if num_hidden_layers > 0 else llm_config.vision_config.num_hidden_layers\n",
    "\n",
    "if num_hidden_layers > llm_config.vision_config.num_hidden_layers:\n",
    "    print(\"Setting num_hidden_layer greater than original model weight will result in randomized weight in the model!\")\n",
    "\n",
    "dtype = torch.float32\n",
    "llm_config.torch_dtype = llm_config.vision_config.torch_dtype = dtype\n",
    "llm_config.vision_config.vision_use_head = False\n",
    "\n",
    "print('num_layer: {}'.format(llm_config.vision_config.num_hidden_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe_logger: Initialize the logger and log environment details \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "recipe_dump_init(output_dir)\n",
    "\n",
    "llm_lib_log_env_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Adapt Gemma3 multi-modal projector\n",
    "\n",
    "The Gemma3 projector in Hugging Face uses MatMul and nn.Parameter for the projection layer, here we replace them with a Conv2d layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gemma3 import modeling_gemma3\n",
    "from gemma3.adaptation import Gemma3MultiModalProjector\n",
    "\n",
    "modeling_gemma3.Gemma3MultiModalProjector = Gemma3MultiModalProjector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Instantiate the HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gemma3 import modeling_gemma3\n",
    "from genai_lib.common.debug.profiler import event_marker\n",
    "\n",
    "with event_marker('HuggingFace FP model creation'):\n",
    "    model = modeling_gemma3.Gemma3ForConditionalGeneration.from_pretrained(model_id, config=llm_config, cache_dir=cache_dir, trust_remote_code=True)\n",
    "\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = '0'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir, use_fast=True, trust_remote_code=True)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id, cache_dir=cache_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Preprocess Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "dataset_path = \"<path to folder containing the coco dataset root folder>\"\n",
    "data_files = \"llm_utils/llava_dataset/llava_v1_5_mix665k_300.json\"\n",
    "\n",
    "class VisionDatasetLoader:\n",
    "    \"\"\"\n",
    "    Dataset for GPTQ-preprocessed tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pixel_values = self.processor(PILImage.open(fp=os.path.join(dataset_path, self.dataset['image'][idx]))).pixel_values[0]\n",
    "        num_channels, height, width = pixel_values.shape\n",
    "        return torch.tensor(pixel_values.reshape(1, num_channels, height, width))\n",
    "\n",
    "with event_marker(\"Load and Preprocess Calibration Data\"):\n",
    "    train_dataset = VisionDatasetLoader(load_dataset(\"json\", data_files=data_files, cache_dir=cache_dir, split='train'), processor)\n",
    "\n",
    "sample_inputs = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Module for VEG (Vision Tower + Projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.utils import place_model\n",
    "\n",
    "class VisualEmbeddingGenerator(torch.nn.Module):\n",
    "    def __init__(self, vision_tower, multi_modal_projector):\n",
    "        super().__init__()\n",
    "        self.multi_modal_projector = multi_modal_projector\n",
    "        self.vision_tower = vision_tower\n",
    "        self.device = vision_tower.device\n",
    "\n",
    "    # this forwrad gets the image pixel values that we get from the AutoProcessor when we pass the image and text (text -> input ids, and image-> pixel values)\n",
    "    # input shape is [1,3,896,896], output shape is [1,256,2560]\n",
    "    def forward(self, pixel_values):\n",
    "        image_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n",
    "        image_features = self.multi_modal_projector(image_outputs)\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sqnr_eval:\n",
    "    veg_orig = VisualEmbeddingGenerator(model.vision_tower, model.multi_modal_projector)\n",
    "    with torch.no_grad(), place_model(veg_orig, torch.device('cuda')):\n",
    "        fp_output = veg_orig(sample_inputs.to(device='cuda')).cpu()  \n",
    "    del veg_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from genai_lib.lvm.dev.transformer.siglip_adaptation import align_siglip_vision_model_tensor_dimensionlity\n",
    "\n",
    "with event_marker('FP model adaptation for NSP backend completion'):\n",
    "    # adaptation for siglip\n",
    "    align_siglip_vision_model_tensor_dimensionlity(model.vision_tower)\n",
    "\n",
    "    # adaptation for projector\n",
    "    model.multi_modal_projector.replace_matmul_with_conv()\n",
    "\n",
    "veg_adapted = VisualEmbeddingGenerator(model.vision_tower, model.multi_modal_projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 SQNR evaluation for adapted FP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQNR of adapted model\n",
    "from genai_lib.lvm.eval_utils import get_sqnr\n",
    "\n",
    "if run_sqnr_eval:\n",
    "    with torch.no_grad(), place_model(veg_adapted, torch.device('cuda')):\n",
    "        output_adapted = veg_adapted(sample_inputs.to(device='cuda')).cpu()\n",
    "\n",
    "    fp_adapted_sqnr = get_sqnr(output_adapted, fp_output)\n",
    "    print(f'Adapted to prepared model SQNR: {fp_adapted_sqnr}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare model using QAIRT model preparer pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from qti.aisw.preparer_api import prepare_model\n",
    "from qti.aisw.emitter.utils.torch_utils import load_torch_model_using_safetensors\n",
    "\n",
    "from genai_lib.llm.model_preparation_utils import llm_build_preparer_converter_args\n",
    "from genai_lib.llm.utils import llm_model_input_output_names\n",
    "\n",
    "\n",
    "skip_prepare = False\n",
    "if skip_prepare:\n",
    "    prepare_path = \"<path to prepared model dir>\"\n",
    "else:\n",
    "    prepare_path = os.path.join(output_dir, 'prepare')\n",
    "os.makedirs(prepare_path, exist_ok=True)\n",
    "prepare_filename = f'{model_name}'\n",
    "\n",
    "if skip_prepare:\n",
    "    with event_marker(f\"KVCache load pre-prepared {prepare_filename}\", flush_ram=True):\n",
    "        prepared_model_path = os.path.join(prepare_path, f'{prepare_filename}.py')\n",
    "        if not os.path.exists(prepared_model_path):\n",
    "            raise ValueError(f\"prepared artifacts not found in {prepare_path}\")\n",
    "        else:\n",
    "            print(f'WARNING: preparation skipped for model={prepare_filename}, prepared at {time.ctime(os.path.getmtime(prepared_model_path))}')\n",
    "            prepared_model = load_torch_model_using_safetensors(path=prepare_path, filename=prepare_filename, model_name=prepare_filename)\n",
    "\n",
    "else:\n",
    "    input_names=['pixel_values']\n",
    "    output_names=['image_features']\n",
    "    \n",
    "    with event_marker(\"Prepare Model\", flush_ram=True):\n",
    "        if __name__ == '__main__': # We use the main guard to prevent child processes from re-running the top-level code\n",
    "            prepared_veg = prepare_model(veg_adapted,\n",
    "                                        sample_inputs.to(device=model.device),\n",
    "                                        model_name=prepare_filename,\n",
    "                                        filename=prepare_filename,\n",
    "                                        path=prepare_path,\n",
    "                                        input_names=input_names,\n",
    "                                        output_names=output_names,\n",
    "                                        onnx_export_args={\"opset_version\":17},\n",
    "                                        keep_original_model_structure=False, # Flatten the model to enable weight-sharing by setting `keep_original_model_structure = False\\n\",\n",
    "                                        order_inputs=True,\n",
    "                                        order_outputs=True,\n",
    "                                        skipped_optimizers=['eliminate_common_subexpression',\n",
    "                                                            'eliminate_nop_with_unit', \n",
    "                                                            'eliminate_duplicate_initializer'\n",
    "                                                            ],\n",
    "                                        return_prepare_model=True,\n",
    "                                        )\n",
    "        else:\n",
    "            raise Exception(\"Killing multiprocessing spawn started by Converter during model preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 SQNR evaluation for prepared FP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sqnr_eval:\n",
    "    with torch.no_grad(), place_model(prepared_veg, torch.device('cuda')):\n",
    "        output_prepared = prepared_veg(sample_inputs.to(device='cuda')).cpu()  \n",
    "    \n",
    "    adapted_prepared_sqnr = get_sqnr(output_adapted, output_prepared)\n",
    "    print(f'Adapted to prepared model SQNR: {adapted_prepared_sqnr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.v2.quantsim import QuantizationSimModel\n",
    "from aimet_torch.v2.experimental import set_matmul_second_input_producer_to_8bit_symmetric, propagate_output_encodings\n",
    "from aimet_torch.nn.modules import custom as elementwise_ops\n",
    "from aimet_torch.v2.experimental.quantsim_utils import clip_weights_to_7f7f\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import functools\n",
    "\n",
    "\n",
    "\n",
    "def copy_model_with_shared_weights(source_model):\n",
    "    target_model = deepcopy(source_model)\n",
    "    for name, source_parameter in source_model.named_parameters():\n",
    "        pre, _, post = name.rpartition('.')\n",
    "        pre_obj = functools.reduce(getattr, [target_model] + pre.split('.')) if pre else target_model\n",
    "        setattr(pre_obj, post, source_parameter)\n",
    "    return target_model\n",
    "\n",
    "# Create copy of fp model defintion for SeqMSE and LoRA Flow\n",
    "fp_prepared_veg = copy_model_with_shared_weights(prepared_veg)\n",
    "\n",
    "with event_marker('Create QuantSim'):\n",
    "    veg_sim = QuantizationSimModel(model=prepared_veg,\n",
    "                                      quant_scheme='tf',\n",
    "                                      dummy_input=sample_inputs.to(device=next(prepared_veg.parameters()).device),\n",
    "                                      default_output_bw=16,\n",
    "                                      default_param_bw=8,\n",
    "                                      in_place=True,\n",
    "                                      config_file=htp_config_file)\n",
    "\n",
    "set_matmul_second_input_producer_to_8bit_symmetric(veg_sim)\n",
    "propagate_output_encodings(veg_sim, elementwise_ops.Concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calibration_batches = 50\n",
    "\n",
    "def _forward_pass(_model, kwargs):\n",
    "    dataset = kwargs['dataset']\n",
    "    num_calibration_batches = kwargs['num_calibration_batches']\n",
    "    with torch.no_grad(), place_model(_model, torch.device('cuda')):\n",
    "        for idx, inputs in enumerate(tqdm(dataset, total=num_calibration_batches)):\n",
    "            if idx > num_calibration_batches:\n",
    "                break\n",
    "            _ = _model(inputs.to(device='cuda'))\n",
    "            \n",
    "kwargs = { \n",
    "            'dataset': train_dataset,\n",
    "            'num_calibration_batches': num_calibration_batches\n",
    "         }\n",
    "    \n",
    "with event_marker(f\"Compute Encodings\"):\n",
    "    with place_model(veg_sim.model, torch.device('cuda')):\n",
    "        veg_sim.compute_encodings(_forward_pass, kwargs)\n",
    "\n",
    "clip_weights_to_7f7f(veg_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 SQNR evaluation for prepared QuantSim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sqnr_eval:\n",
    "    with torch.no_grad(), place_model(veg_sim.model, torch.device(\"cuda\")):\n",
    "        sim_output = veg_sim.model(sample_inputs.to(device='cuda')).cpu()\n",
    "    \n",
    "    prepared_sim_sqnr = get_sqnr(output_prepared, sim_output)\n",
    "    print(f'Prepared to sim model SQNR: {prepared_sim_sqnr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export\n",
    "the pipeline call below would export onnx model, encoding and test vector for KVCache models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Generate Test Vectors for QNN SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai_lib.lvm.eval_utils import generate_vectors\n",
    "\n",
    "base_output_dir = os.path.join(output_dir, 'export')\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "with event_marker(f\"Vector generation\"):\n",
    "    generate_vectors(output_path = os.path.join(base_output_dir, 'test_vectors'),\n",
    "                     sim_model = veg_sim.model.to(device='cuda',dtype=dtype),\n",
    "                     sample_inputs = [sample_inputs.to(device='cuda')],\n",
    "                     input_names = input_names,\n",
    "                     output_names = output_names,\n",
    "                     num_samples = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch import onnx_utils\n",
    "from aimet_torch.onnx_utils import OnnxExportApiArgs\n",
    "\n",
    "\n",
    "# Setting this flag to False means that the prepared model will be flattened\n",
    "onnx_utils.EXPORT_TO_ONNX_DIRECT = True\n",
    "\n",
    "base_onnx_dir = os.path.join(base_output_dir, 'onnx')\n",
    "os.makedirs(base_onnx_dir, exist_ok=True)\n",
    "with event_marker('Export ONNX and Encodings'):\n",
    "    onnx_api_args = OnnxExportApiArgs(opset_version=17, input_names=input_names, output_names=output_names)\n",
    "    # onnx_utils.RESTORE_ONNX_MODEL_INITIALIZERS = True\n",
    "    veg_sim.model.cpu()\n",
    "    veg_sim.export(path=base_onnx_dir, filename_prefix=model_name, dummy_input=sample_inputs, onnx_export_args=onnx_api_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Save Quantsim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Increase recursion depth limit to save full model\n",
    "sys.setrecursionlimit(100000)\n",
    "\n",
    "# base_dir = os.path.join(output_dir, 'quantsim')\n",
    "with event_marker(\"save quantsim model\"), open(f\"{output_dir}/{prepare_filename}.pkl\", 'wb') as file:\n",
    "    pkl.dump(veg_sim, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "from genai_lib.common.debug.profiler import EventProfiler\n",
    "from genai_lib.common.debug.recipe_logger import dump_logs_to_json\n",
    "\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(output_dir, 'profiling_stats.json'))\n",
    "dump_logs_to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Qualcomm Technologies, Inc. and/or its subsidiaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
