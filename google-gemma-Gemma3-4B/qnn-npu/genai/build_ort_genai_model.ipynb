{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aeb1ec1-007d-4feb-98ff-754978bbe90d",
   "metadata": {},
   "source": [
    "# Generate ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab83b2",
   "metadata": {},
   "source": [
    "Download the gen_qnn_ctx_onnx_model.py script from the onnxruntime repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7892a",
   "metadata": {},
   "source": [
    "## Extract the model data from the model.bin file using the qnn sdks utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5960b",
   "metadata": {},
   "source": [
    "### Setting environment variables names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfe2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "QNN_SDK_DIR = r\"C:\\Qualcomm\\AIStack\\QAIRT\\2.40.0.251030\"\n",
    "os.environ['QNN_ROOT']=f\"{QNN_SDK_DIR}\"\n",
    "os.environ['PYTHONPATH']= f\"{QNN_SDK_DIR}\\\\lib\\\\python\"\n",
    "os.environ['PATH']=os.environ['PATH']+f\"{QNN_SDK_DIR}\\\\bin\\\\aarch64-windows-msvc\"\n",
    "os.environ['TMPDIR']=r\"C:\\Users\\HCKTest\\AppData\\Local\\Temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadc467",
   "metadata": {},
   "source": [
    "### Run the qnn-context-binary-utility.exe script to get the model JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0ddbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "bin_files = glob.glob(os.path.join(os.getcwd(),'*serialized.bin'))\n",
    "json_files = [s.replace(\"bin\", \"json\") for s in bin_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99de38b4-7443-4dd3-9ca7-b1a52e96d20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\chilukam\\\\Gemma\\\\IOT\\\\artifacts_from_code\\\\artifacts_testing\\\\veg.serialized.bin',\n",
       " 'C:\\\\chilukam\\\\Gemma\\\\IOT\\\\artifacts_from_code\\\\artifacts_testing\\\\weight_sharing_model_1_of_4.serialized.bin',\n",
       " 'C:\\\\chilukam\\\\Gemma\\\\IOT\\\\artifacts_from_code\\\\artifacts_testing\\\\weight_sharing_model_2_of_4.serialized.bin',\n",
       " 'C:\\\\chilukam\\\\Gemma\\\\IOT\\\\artifacts_from_code\\\\artifacts_testing\\\\weight_sharing_model_3_of_4.serialized.bin',\n",
       " 'C:\\\\chilukam\\\\Gemma\\\\IOT\\\\artifacts_from_code\\\\artifacts_testing\\\\weight_sharing_model_4_of_4.serialized.bin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a2d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_cmd = ''\n",
    "for bin_file, json_file in zip(bin_files, json_files):\n",
    "    generator_cmd += \"& ${QNN_SDK_ROOT}\\\\bin\\\\x86_64-windows-msvc\\\\qnn-context-binary-utility.exe\"+f\" --context_binary {bin_file} --json_file {json_file}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "587de062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "envsetup = os.path.join(QNN_SDK_DIR, \"bin\", \"envsetup.ps1\")\n",
    "command = [f\"& {envsetup}\", generator_cmd]\n",
    "powershell_script = '\\n'.join(command)\n",
    "\n",
    "try:\n",
    "    subprocess.run([\"powershell.exe\", \"-Command\", powershell_script], check=True, capture_output=True, text=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error Output:\\n\", e.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad4931",
   "metadata": {},
   "source": [
    "## Generate ONNX wrapper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fe5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_env_path = os.path.join(os.getcwd(),\"Qairt_Env\\\\Scripts\\\\Activate.ps1\")\n",
    "gen_qnn_ctx_onnx_cmd = (\n",
    "    'Get-ChildItem -Filter \"weight_sharing_model*.bin\" | ForEach-Object { '\n",
    "    '$binFile = $_.Name; '\n",
    "    '$jsonFile = \\\"$($binFile -replace \\'.bin$\\', \\'.json\\')\\\"; '\n",
    "    'python gen_qnn_ctx_onnx_model.py -b $binFile -q $jsonFile --quantized_IO --disable_embed_mode '\n",
    "    '}'\n",
    ")\n",
    "command = f\"& {python_env_path}; {gen_qnn_ctx_onnx_cmd}\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"powershell.exe\", \"-Command\", command],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45920489-f09b-42a2-9dc4-f1c98df1f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_env_path = os.path.join(os.getcwd(),\"Qairt_Env\\\\Scripts\\\\Activate.ps1\")\n",
    "gen_qnn_ctx_onnx_cmd = (\n",
    "    'Get-ChildItem -Filter \"veg*.bin\" | ForEach-Object { '\n",
    "    '$binFile = $_.Name; '\n",
    "    '$jsonFile = \\\"$($binFile -replace \\'.bin$\\', \\'.json\\')\\\"; '\n",
    "    'python gen_qnn_ctx_onnx_model.py -b $binFile -q $jsonFile --disable_embed_mode '\n",
    "    '}'\n",
    ")\n",
    "command = f\"& {python_env_path}; {gen_qnn_ctx_onnx_cmd}\"\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"powershell.exe\", \"-Command\", command],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaca1a8-e4a2-4c84-883b-cc8353820a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1158eeb-c0e7-4a19-a5db-c47d9d48de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ff575c-9c1d-4cc6-adb1-f21eaa56a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_common = ['swa_position_ids_cos', 'swa_position_ids_sin', 'swa_attention_mask', 'position_ids_cos', 'position_ids_sin', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41c298d1-6763-4c98-8dc2-3cd775ee82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mods(m1, m2):\n",
    "    input_output = [x.name for x in m1.graph.output][-1]\n",
    "    common = [input_output, *all_common]\n",
    "    m1.graph.node.extend([x for x in m2.graph.node if x.name not in common])\n",
    "    m1_output_ind, m1_output_vi = next(((i, x) for i, x in enumerate(m1.graph.output) if x.name == input_output))\n",
    "    m1.graph.input.extend([x for x in m2.graph.input if x.name not in common])\n",
    "    m1.graph.output.extend([x for x in m2.graph.output if x.name not in common])\n",
    "    return m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb34a2e-7d01-4d13-a263-ca1442769f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ar, cl in zip([1, 128], [8192, 8192]):\n",
    "    m1 = onnx.load(f\"ar{ar}_cl{cl}_1_of_4_qnn_ctx.onnx\")\n",
    "    m2 = onnx.load(f\"ar{ar}_cl{cl}_2_of_4_qnn_ctx.onnx\")\n",
    "    m3 = onnx.load(f\"ar{ar}_cl{cl}_3_of_4_qnn_ctx.onnx\")\n",
    "    m4 = onnx.load(f\"ar{ar}_cl{cl}_4_of_4_qnn_ctx.onnx\")\n",
    "    for m in [m2, m3, m4]:\n",
    "        m1 = combine_mods(m1, m)\n",
    "    onnx.save(m1, f\"ar{ar}_cl{cl}_all_of_4_qnn_ctx.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec3073-72c6-42d6-8756-c0b61f8b62ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10565386",
   "metadata": {},
   "source": [
    "# Generate Position Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89dc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c8530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gemma3.modeling_gemma3 import (Gemma3TextModel as Model, Gemma3TextConfig as Config, Gemma3RotaryEmbedding as RotaryEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a458ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 128\n",
    "context_length = 8192\n",
    "config_file_path = os.path.join(os.getcwd(), 'config.json')\n",
    "config = Config.from_pretrained(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed39d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qualcomm_prepare_4d_causal_sliding_window_attention_mask_with_cache_position(\n",
    "        attention_mask: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        target_length: int,\n",
    "        dtype: torch.dtype,\n",
    "        cache_position: torch.Tensor,\n",
    "        batch_size: int,\n",
    "        sliding_window: int,\n",
    "    )->torch.Tensor:\n",
    "    min_dtype = -50\n",
    "    causal_mask = torch.full(\n",
    "        (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n",
    "    )\n",
    "    if sequence_length != 1:\n",
    "        causal_mask = torch.triu(causal_mask, diagonal=target_length-sequence_length)\n",
    "    causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n",
    "    causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "        mask_length = attention_mask.shape[-1]\n",
    "        padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n",
    "            causal_mask.device\n",
    "        )\n",
    "        padding_mask = padding_mask == 0\n",
    "        causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "            padding_mask, min_dtype\n",
    "        )\n",
    "    swa_causal_mask = causal_mask[:, :, :, -sliding_window:]\n",
    "    return causal_mask, swa_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78e73c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(Model, \"_prepare_4d_causal_sliding_window_attention_mask_with_cache_position\", Qualcomm_prepare_4d_causal_sliding_window_attention_mask_with_cache_position)\n",
    "\n",
    "class PositionProcessor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.rotary_emb = RotaryEmbedding(config)\n",
    "        config = copy.deepcopy(config)\n",
    "        self.sliding_window = config.sliding_window\n",
    "        config.rope_theta = config.rope_local_base_freq\n",
    "        config.rope_scaling = {\"rope_type\": \"default\"}\n",
    "        self.rotary_emb_local = RotaryEmbedding(config)\n",
    "\n",
    "    def forward(self, attention_mask: torch.Tensor, position_ids: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # head_dim = self.config.hidden_size//self.config.num_attention_heads\n",
    "        head_dim = self.config.head_dim\n",
    "        batch_size, seq_len = position_ids.shape\n",
    "        context_length = attention_mask.shape[1]\n",
    "\n",
    "        cache_position = torch.arange(context_length-seq_len, context_length, device='cpu')\n",
    "        \n",
    "        causal_mask_global, causal_mask_local = Model._prepare_4d_causal_sliding_window_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=seq_len,\n",
    "            target_length=context_length,\n",
    "            dtype=torch.float32,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=batch_size,\n",
    "            sliding_window = self.sliding_window\n",
    "        )\n",
    "\n",
    "        # Rotary embeddings Global\n",
    "        dummy_tensor = torch.zeros((batch_size, seq_len, self.config.hidden_size), device=position_ids.device)\n",
    "        cos, sin = self.rotary_emb(dummy_tensor, position_ids)\n",
    "\n",
    "        cos = cos[:1, :seq_len, :head_dim//2].unsqueeze(0)\n",
    "        sin = sin[:1, :seq_len, :head_dim//2].unsqueeze(0)\n",
    "\n",
    "        cos_local, sin_local = self.rotary_emb_local(dummy_tensor, position_ids)\n",
    "\n",
    "        cos_local = cos_local[:1, :seq_len, :head_dim//2].unsqueeze(0)\n",
    "        sin_local = sin_local[:1, :seq_len, :head_dim//2].unsqueeze(0)\n",
    "\n",
    "        return causal_mask_global, causal_mask_local, cos, sin, cos_local, sin_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feaa9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_pretrained(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4be0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and model\n",
    "\n",
    "model = PositionProcessor(config)\n",
    "model.eval()\n",
    "\n",
    "# Dummy inputs\n",
    "attention_mask = torch.ones((batch_size, context_length), dtype=torch.int32)\n",
    "position_ids = torch.arange(seq_len, dtype=torch.int32).unsqueeze(0)\n",
    "\n",
    "outputs = model(attention_mask, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34799bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model 'position-processor_swa.onnx' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCKTest\\AppData\\Local\\Temp\\ipykernel_17108\\3112237263.py:2: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter has become the default. Learn more about the new export logic: https://docs.pytorch.org/docs/stable/onnx_export.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html\n",
      "  torch.onnx.export(\n",
      "C:\\chilukam\\Gemma\\IOT\\artifacts_from_code\\gemma_env\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\utils.py:552: OnnxExporterWarning: Exporting to ONNX opset version 21 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 20. To use a newer opset version, consider 'torch.onnx.export(..., dynamo=True)'. \n",
      "  _export(\n",
      "C:\\Users\\HCKTest\\AppData\\Local\\Temp\\ipykernel_17108\\1085862225.py:14: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    }
   ],
   "source": [
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (attention_mask, position_ids),\n",
    "    \"position-processor_swa_same_attn.onnx\",\n",
    "    input_names=[\"attention_mask_before_processor\", \"position_ids\"],\n",
    "    output_names=[\"attention_mask_before_quantizer\", \"swa_attention_mask_before_quantizer\", \n",
    "                  \"position_ids_cos_before_quantizer\", \"position_ids_sin_before_quantizer\",\n",
    "                 \"swa_position_ids_cos_before_quantizer\", \"swa_position_ids_sin_before_quantizer\"],\n",
    "    dynamic_axes={\n",
    "        \"attention_mask_before_processor\": {1: \"ctx_len\"},\n",
    "        \"position_ids\": {1: \"seq_len\"},\n",
    "        \"attention_mask_before_quantizer\": {1: \"seq_len\"},\n",
    "        \"position_ids_cos_before_quantizer\": {2: \"seq_len\"},\n",
    "        \"position_ids_sin_before_quantizer\": {2: \"seq_len\"},\n",
    "    },\n",
    "    opset_version=21,\n",
    "    dynamo = False\n",
    ")\n",
    "print(\"ONNX model 'position-processor_swa.onnx' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806d453-7975-406f-a419-e7326e0ebb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d213d91-17a8-4ed4-a3ba-e05a26217f26",
   "metadata": {},
   "source": [
    "# Add Qunatize Layer at output of position processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdfbb10a-b19f-44a7-a82c-674f9522fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_json_file = json.load(open('weight_sharing_model_1_of_4.serialized.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "937aab41-377e-4552-9de7-be85c1d93e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0015259021893143654, 65535, 3.051804378628731e-05, 32768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scale = None\n",
    "attn_offset = None\n",
    "pos_scale = None\n",
    "pos_offset = None\n",
    "graph_inputs = model_json_file['info']['graphs'][0]['info']['graphInputs']\n",
    "    \n",
    "for input in graph_inputs:\n",
    "    if input['info']['name'] == 'attention_mask':\n",
    "        attn_scale = input['info']['quantizeParams']['scaleOffset']['scale']\n",
    "        attn_offset = abs(input['info']['quantizeParams']['scaleOffset']['offset'])\n",
    "    elif input['info']['name'] == 'position_ids_cos':\n",
    "        pos_scale = input['info']['quantizeParams']['scaleOffset']['scale']\n",
    "        pos_offset = abs(input['info']['quantizeParams']['scaleOffset']['offset'])\n",
    "    else:\n",
    "        if attn_scale and attn_offset and pos_scale and pos_offset:\n",
    "            break\n",
    "attn_scale, attn_offset, pos_scale, pos_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806422d-68cf-40c0-b563-3a357b097e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2f66f9f-2ab9-44e9-95fd-8a783405beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import onnx\n",
    "import numpy as np\n",
    "from onnx import helper, TensorProto, checker\n",
    "\n",
    "def _tensor_shape_from_value_info(vi):\n",
    "    \"\"\"Extract shape as a list of ints/strings/None from a ValueInfoProto.\"\"\"\n",
    "    t = vi.type.tensor_type\n",
    "    if not t.HasField(\"shape\"):\n",
    "        return None\n",
    "    dims = []\n",
    "    for d in t.shape.dim:\n",
    "        if d.HasField(\"dim_param\") and d.dim_param:\n",
    "            dims.append(d.dim_param)  # symbolic dim\n",
    "        elif d.HasField(\"dim_value\"):\n",
    "            dims.append(d.dim_value)  # concrete dim\n",
    "        else:\n",
    "            dims.append(None)         # unknown\n",
    "    return dims\n",
    "\n",
    "def _get_opset(model, domain=\"\"):\n",
    "    for o in model.opset_import:\n",
    "        if o.domain == domain:\n",
    "            return o\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fcb2ae7-515b-431b-b50e-fe80465bf541",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"position-processor_swa_same_attn.onnx\"\n",
    "output_path = \"position-processor_swa_same_attn_quant.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d649a6-4ed3-4d45-9f6c-cd835089a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(input_path)\n",
    "graph = model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8363e49f-6e97-427c-8645-ff1d6013c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output_names = [vi.name for vi in graph.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a7fe328-2f7b-42ab-afac-482efa69f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_output_names = graph_output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddfaf5be-192d-4656-8c24-f2c1be30774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scale_values = [attn_scale, attn_scale, pos_scale, pos_scale, pos_scale, pos_scale] \n",
    "x_zero_point_values = [attn_offset, attn_offset, pos_offset, pos_offset, pos_offset, pos_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9da61120-0bc1-4121-8855-c0adf2fb98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_names = [old_output_name.replace('_before_quantizer', '') for old_output_name in old_output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63082414-08b1-45f4-b313-942a560da5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_node_names = [new_output_name+\"_quant\" for new_output_name in new_output_names]\n",
    "q_input_names = [new_output_name+\"_in\" for new_output_name in new_output_names]\n",
    "x_scale_names = [new_output_name+\"_scale\" for new_output_name in new_output_names]\n",
    "x_zero_point_names = [new_output_name+\"_zp\" for new_output_name in new_output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb6104c3-0890-454a-b987-23425f143b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_input_vis = [next((vi for vi in graph.output if vi.name == old_output_name), None) for old_output_name in old_output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25b66241-4bf1-4350-85b2-73903420277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shapes = [_tensor_shape_from_value_info(old_input_vi) for old_input_vi in old_input_vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb565cdc-ca8a-4e8b-9a2d-a5d902406f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_elem = TensorProto.UINT16\n",
    "zp_np_dtype = np.uint16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12d9df25-cd89-4459-a907-56ef3a9bcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_in_use(name: str) -> bool:\n",
    "    if name in graph_output_names:\n",
    "        return True\n",
    "    if name in [o.name for o in graph.input]:\n",
    "        return True\n",
    "    if name in [init.name for init in graph.initializer]:\n",
    "        return True\n",
    "    for n in graph.node:\n",
    "        if name in list(n.input) + list(n.output):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0228c77d-a6e6-4b8b-8a19-56f664441e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for must_be_unique in [*new_output_names, *q_input_names, *x_scale_names, *x_zero_point_names]:\n",
    "    if name_in_use(must_be_unique):\n",
    "        raise ValueError(f\"Name '{must_be_unique}' already exists in the graph. \"\n",
    "                         f\"Please provide a different name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "792f935c-c223-4f5f-95b0-94acf1d16155",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_vis = [helper.make_tensor_value_info(new_output_name, q_elem, old_shape) for (new_output_name, old_shape) in zip(new_output_names, old_shapes)]\n",
    "graph.output.extend(new_output_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a36cd93-d77f-4fc6-a35f-0bdce66acbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_nps = [np.array([x_scale_value], dtype=np.float32) for x_scale_value in x_scale_values]\n",
    "zp_nps = [np.array([x_zero_point_value], dtype=zp_np_dtype) for x_zero_point_value in x_zero_point_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41d0ab50-b862-4d9f-9645-d9a730f00f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scale_inits = [helper.make_tensor(name=x_scale_name, data_type=TensorProto.FLOAT, dims=(1,), vals=scale_np) \\\n",
    "                 for x_scale_name, scale_np in zip(x_scale_names, scale_nps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b9f9ee8-16cf-40d6-b94b-569fb92c4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_zero_point_inits = [helper.make_tensor(name=x_zero_point_name, data_type=q_elem, dims=(1,), vals=zp_np) \\\n",
    "                 for x_zero_point_name, zp_np in zip(x_zero_point_names, zp_nps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c54c877-4b7c-4af5-8ba1-58f25ab343f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.initializer.extend([*x_scale_inits, *x_zero_point_inits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfbd8f54-fa76-4c3e-b4a7-73dd6f82c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_nodes = [helper.make_node(\"QuantizeLinear\", inputs=[q_input_name, x_scale_name, x_zero_point_name], outputs=[new_output_name], name=q_node_name) \\\n",
    "        for new_output_name, x_scale_name, x_zero_point_name, q_input_name, q_node_name in \\\n",
    "        zip(new_output_names, x_scale_names, x_zero_point_names, q_input_names, q_node_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0965c3a-f411-4100-ae32-125074d54121",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.node.extend(q_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d341073d-cb76-4e65-adb8-0a2e51029d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.node:\n",
    "    for i, inp in enumerate(node.output):\n",
    "        if inp in old_output_names:\n",
    "            new_ind = old_output_names.index(inp)\n",
    "            node.output[i] = q_input_names[new_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78079a4c-c049-4a3e-a388-85f206f19160",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.node:\n",
    "    for i, inp in enumerate(node.input):\n",
    "        if inp in old_output_names:\n",
    "            new_ind = old_output_names.index(inp)\n",
    "            node.input[i] = q_input_names[new_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96902bb3-d019-4a8d-8db2-f7ebc5fcd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_idxs = []\n",
    "for i, vi in enumerate(graph.output):\n",
    "    if vi.name in old_output_names:\n",
    "        old_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25be0b7f-a000-43ba-863a-faa1ddae6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for old_idx in old_idxs[::-1]:\n",
    "    del graph.output[old_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db597895-46dc-4e60-a1dd-faff860034d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version: 21"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opset = _get_opset(model, \"\")\n",
    "opset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4115a7cb-a6cb-4d2a-8fbb-9942f99773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker.check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdc8ba1d-a0db-43dc-8dc0-9a82197a952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f284e-860a-46fb-84b2-541ea2365542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39cfd2fb-4d96-4968-b05e-538d5b3a61f2",
   "metadata": {},
   "source": [
    "# Add Quantize Layer to Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5d3f9ad9-1688-423c-974c-9ea21de4de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_json_file = json.load(open('weight_sharing_model_1_of_4.serialized.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3c4dba2d-6f5c-474b-b6b8-fbd8ae63c3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0009816634701564908, 29193)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scale = None\n",
    "attn_offset = None\n",
    "pos_scale = None\n",
    "pos_offset = None\n",
    "graph_inputs = model_json_file['info']['graphs'][0]['info']['graphInputs']\n",
    "    \n",
    "for input in graph_inputs:\n",
    "    if input['info']['name'] == 'inputs_embeds':\n",
    "        embed_scale = input['info']['quantizeParams']['scaleOffset']['scale']\n",
    "        embed_offset = abs(input['info']['quantizeParams']['scaleOffset']['offset'])\n",
    "        break\n",
    "embed_scale, embed_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984b653-55ea-4f85-a60b-0cec47d43584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7cd55877-62ec-4585-b5f6-42ca58f9f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import onnx\n",
    "import numpy as np\n",
    "from onnx import helper, TensorProto, checker\n",
    "\n",
    "def _tensor_shape_from_value_info(vi):\n",
    "    \"\"\"Extract shape as a list of ints/strings/None from a ValueInfoProto.\"\"\"\n",
    "    t = vi.type.tensor_type\n",
    "    if not t.HasField(\"shape\"):\n",
    "        return None\n",
    "    dims = []\n",
    "    for d in t.shape.dim:\n",
    "        if d.HasField(\"dim_param\") and d.dim_param:\n",
    "            dims.append(d.dim_param)  # symbolic dim\n",
    "        elif d.HasField(\"dim_value\"):\n",
    "            dims.append(d.dim_value)  # concrete dim\n",
    "        else:\n",
    "            dims.append(None)         # unknown\n",
    "    return dims\n",
    "\n",
    "def _get_opset(model, domain=\"\"):\n",
    "    for o in model.opset_import:\n",
    "        if o.domain == domain:\n",
    "            return o\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3e37fccb-cebe-4529-a597-0d14c2810b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"embed_fp32.onnx\"\n",
    "output_path = \"embed_fp32_mod.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2704b0f9-5877-4d9d-af23-ceba8d91e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(input_path)\n",
    "graph = model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c6bc9ebf-31e5-435c-91b3-69b152a13381",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output_names = [vi.name for vi in graph.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c31dfd3b-5250-4db5-a70e-f6525a801847",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_output_names = graph_output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b721d689-b6ae-4db4-87c5-30743cf0f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scale_values = [embed_scale] \n",
    "x_zero_point_values = [embed_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2c5ba826-d1e4-418c-a350-7cff9e1e32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_names = [\"inputs_embeds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1da30d09-43fd-44de-a176-3c94e2b70072",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_node_names = [new_output_name+\"_quant\" for new_output_name in new_output_names]\n",
    "q_input_names = [new_output_name+\"_in\" for new_output_name in new_output_names]\n",
    "x_scale_names = [new_output_name+\"_scale\" for new_output_name in new_output_names]\n",
    "x_zero_point_names = [new_output_name+\"_zp\" for new_output_name in new_output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "91b34abf-ec62-4edb-af9d-71ce3ee1a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_input_vis = [next((vi for vi in graph.output if vi.name == old_output_name), None) for old_output_name in old_output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d4851d5c-c02b-4af2-8eb4-4467263957cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shapes = [_tensor_shape_from_value_info(old_input_vi) for old_input_vi in old_input_vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a3afb8f2-ff0a-4db5-a228-f8aa30d3e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_elem = TensorProto.UINT16\n",
    "zp_np_dtype = np.uint16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b5c6858d-86a1-4103-b62f-825bd4c45214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_in_use(name: str) -> bool:\n",
    "    if name in graph_output_names:\n",
    "        return True\n",
    "    if name in [o.name for o in graph.input]:\n",
    "        return True\n",
    "    if name in [init.name for init in graph.initializer]:\n",
    "        return True\n",
    "    for n in graph.node:\n",
    "        if name in list(n.input) + list(n.output):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "12726232-c89f-469d-a0cb-3e721f168b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for must_be_unique in [*new_output_names, *q_input_names, *x_scale_names, *x_zero_point_names]:\n",
    "    if name_in_use(must_be_unique):\n",
    "        raise ValueError(f\"Name '{must_be_unique}' already exists in the graph. \"\n",
    "                         f\"Please provide a different name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e835a4c5-106e-430f-9292-d1e2afddc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_vis = [helper.make_tensor_value_info(new_output_name, q_elem, old_shape) for (new_output_name, old_shape) in zip(new_output_names, old_shapes)]\n",
    "graph.output.extend(new_output_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ddbcb90b-7df4-463b-861b-82458045f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_nps = [np.array([x_scale_value], dtype=np.float32) for x_scale_value in x_scale_values]\n",
    "zp_nps = [np.array([x_zero_point_value], dtype=zp_np_dtype) for x_zero_point_value in x_zero_point_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6e22a8a7-b981-4d4d-b153-507ff2dd216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scale_inits = [helper.make_tensor(name=x_scale_name, data_type=TensorProto.FLOAT, dims=(1,), vals=scale_np) \\\n",
    "                 for x_scale_name, scale_np in zip(x_scale_names, scale_nps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "01206969-c233-4e47-a183-8de0f54d1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_zero_point_inits = [helper.make_tensor(name=x_zero_point_name, data_type=q_elem, dims=(1,), vals=zp_np) \\\n",
    "                 for x_zero_point_name, zp_np in zip(x_zero_point_names, zp_nps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "db5e4103-ad8f-41f5-a15f-4fe02f8a340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.initializer.extend([*x_scale_inits, *x_zero_point_inits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "fce8cd61-af1b-4ea3-a607-8220c88fab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_nodes = [helper.make_node(\"QuantizeLinear\", inputs=[q_input_name, x_scale_name, x_zero_point_name], outputs=[new_output_name], name=q_node_name) \\\n",
    "        for new_output_name, x_scale_name, x_zero_point_name, q_input_name, q_node_name in \\\n",
    "        zip(new_output_names, x_scale_names, x_zero_point_names, q_input_names, q_node_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "47c7717b-4716-42d0-93b7-7d063e31a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.node.extend(q_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4db526b8-d311-4158-8ad5-05bfc58e8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.node:\n",
    "    for i, inp in enumerate(node.output):\n",
    "        if inp in old_output_names:\n",
    "            new_ind = old_output_names.index(inp)\n",
    "            node.output[i] = q_input_names[new_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d645d866-88b9-4604-974b-b28ab425df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in graph.node:\n",
    "    for i, inp in enumerate(node.input):\n",
    "        if inp in old_output_names:\n",
    "            new_ind = old_output_names.index(inp)\n",
    "            node.input[i] = q_input_names[new_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fa6fa7a5-dea1-43d2-9fdb-729e0f2cab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_idxs = []\n",
    "for i, vi in enumerate(graph.output):\n",
    "    if vi.name in old_output_names:\n",
    "        old_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "75316a82-9c3c-4e4d-9990-bf7a085bb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for old_idx in old_idxs[::-1]:\n",
    "    del graph.output[old_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "db309fe8-a31e-4819-bc8e-52e21be24f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "version: 19"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opset = _get_opset(model, \"\")\n",
    "opset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2561b237-79a2-44f6-b1b3-586ba05484d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opset.version = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b88388-28fc-4935-8dc5-97052c4f8530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "97a260eb-a209-431b-ad3e-966649d09016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_input = copy.deepcopy(model.graph.input[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e9b724cb-48af-4c35-98db-693a9006ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_input_name = \"image_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "882c3d03-d7d3-445f-9f43-3c766a0c36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input.name = \"image_features_dq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "bf97d3ad-07d7-40a6-b42c-6c1869bf5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.graph.input.extend([new_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7616e871-52fd-4f20-8848-e0a33d565e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in model.graph.node:\n",
    "    for i, input_name in enumerate(node.input):\n",
    "        if input_name == old_input_name:\n",
    "            node.input[i] = new_input.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "8bb23332-9d45-4d36-b394-46a921368606",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_idx = next((i for i, vi in enumerate(model.graph.input) if vi.name == old_input_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a1a1483d-225a-404f-af69-bca2ecabbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.graph.input[input_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "85fd5d6a-7cc4-43fe-967a-50b3668db4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(model, output_path, all_tensors_to_one_file=True, save_as_external_data=True, location = \"embed_fp32_mod.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "54651725-90a9-490a-a833-174df54937c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker.check_model(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b46b6a-b0b1-4e77-a350-4c21cdd394a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40987a95-1557-4291-bdad-2a71502710b2",
   "metadata": {},
   "source": [
    "# Create Ouptut Dequantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5117d73e-8200-4902-b649-71f4003b51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import onnx.helper as helper\n",
    "from onnx import helper, TensorProto\n",
    "import onnx.numpy_helper as numpy_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3b01a4d3-fc75-40f8-9373-cbd643043ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dequantizer_onnx()->None:\n",
    "    # Define the input tensor\n",
    "    input_tensor = helper.make_tensor_value_info(\n",
    "        'logits', TensorProto.UINT16, [1, 'sequence_length', 'vocab_size']\n",
    "    )\n",
    "\n",
    "    # Define the output tensor\n",
    "    output_tensor = helper.make_tensor_value_info(\n",
    "        'logits_dequantized', TensorProto.FLOAT, [1, 'sequence_length', 'vocab_size']\n",
    "    )\n",
    "\n",
    "    # Create the Cast node to convert uint16 to float32\n",
    "    cast_node = helper.make_node(\n",
    "        'Cast',\n",
    "        inputs=['logits'],\n",
    "        outputs=['logits_dequantized'],\n",
    "        to=TensorProto.FLOAT,\n",
    "        name='logits_dequantizer'\n",
    "    )\n",
    "\n",
    "    # Create the graph\n",
    "    graph_def = helper.make_graph(\n",
    "        nodes=[cast_node],\n",
    "        name='dequantizer',\n",
    "        inputs=[input_tensor],\n",
    "        outputs=[output_tensor]\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    model_def = helper.make_model(graph_def, producer_name='onnx-dequantize-example')\n",
    "    model_def.ir_version = 10\n",
    "    model_def.opset_import[0].version = 21\n",
    "    # Save the model\n",
    "    onnx.save(model_def, 'dequantizer.onnx')\n",
    "\n",
    "    print(\"ONNX model 'dequantize.onnx' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fcb31fd3-69d9-49b5-b22f-ebb32c74ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model 'dequantize.onnx' created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_dequantizer_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f26c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d55d928b-c8a7-4cd5-a4c1-1deb65a0b525",
   "metadata": {},
   "source": [
    "# Input Ops for SWA to Past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c527dad4-a400-47d9-bb10-325f400e374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bd19ff3a-1ad1-4275-aa32-1f9706d8b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSWAMod(torch.nn.Module):\n",
    "    def __init__(self, swa_dim, past_dim):\n",
    "        super().__init__()\n",
    "        self.swa_dim = swa_dim\n",
    "        self.past_dim = past_dim\n",
    "\n",
    "    def forward(self, past_key_ins, past_value_ins):\n",
    "        swa_past_key_ins = []\n",
    "        swa_past_value_ins = []\n",
    "        \n",
    "        for ind, past_key_in in enumerate(past_key_ins):\n",
    "            # past_key_in = past_key_in.transpose(0, 1)\n",
    "            past_key_in = past_key_in.view((1, 4, 256, self.past_dim))\n",
    "            if (ind+1)%6 != 0: \n",
    "                swa_past_key_ins.append(past_key_in[:, :, :, self.past_dim-self.swa_dim:self.past_dim])\n",
    "            else: \n",
    "                swa_past_key_ins.append(past_key_in)\n",
    "                \n",
    "        for ind, past_value_in in enumerate(past_value_ins):\n",
    "            # past_value_in = past_value_in.transpose(0, 1)\n",
    "            past_value_in = past_value_in.view((1, 4, self.past_dim, 256))\n",
    "            if (ind+1)%6 != 0: \n",
    "                swa_past_value_ins.append(past_value_in[:, :, self.past_dim-self.swa_dim:past_dim, :])\n",
    "            else:\n",
    "                swa_past_value_ins.append(past_value_in)\n",
    "\n",
    "        return swa_past_key_ins, swa_past_value_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f0d8d883-8908-4594-b782-39b8a6693182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCKTest\\AppData\\Local\\Temp\\ipykernel_17108\\291240081.py:27: UserWarning: Exporting a model while it is in training mode. Please ensure that this is intended, as it may lead to different behavior during inference. Calling model.eval() before export is recommended.\n",
      "  torch.onnx.export(\n",
      "W0212 10:51:22.848000 17108 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0212 10:51:22.849000 17108 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0212 10:51:22.850000 17108 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `InputSWAMod()` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `InputSWAMod()` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCKTest\\.pyenv\\pyenv-win\\versions\\3.12.10-arm\\Lib\\copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "ONNX model 'input-processor_swa.onnx' created successfully.\n"
     ]
    }
   ],
   "source": [
    "context_length = 8192\n",
    "past_dim = context_length - 128\n",
    "swa_dim = 896\n",
    "swa_input_mod = InputSWAMod(swa_dim, past_dim)\n",
    "\n",
    "past_key_shape = (4, 1, 256, past_dim)\n",
    "past_value_shape = (4, 1, past_dim, 256)\n",
    "\n",
    "past_key_inputs = [torch.zeros(past_key_shape, dtype = torch.uint8) for i in range(34)]\n",
    "past_value_inputs = [torch.zeros(past_value_shape, dtype = torch.uint8) for i in range(34)]\n",
    "\n",
    "output_key, output_value = swa_input_mod(past_key_inputs, past_value_inputs)\n",
    "\n",
    "own_past_key_pattern = \"own_past_key_%d_in\"\n",
    "own_past_value_pattern = \"own_past_value_%d_in\"\n",
    "\n",
    "past_key_pattern = \"past_key_%d_in\"\n",
    "past_value_pattern = \"past_value_%d_in\"\n",
    "swa_key_pattern = \"swa_key_%d_in\"\n",
    "swa_value_pattern = \"swa_value_%d_in\"\n",
    "\n",
    "input_names = [own_past_key_pattern%(i) for i in range(34)] + [own_past_value_pattern%(i) for i in range(34)]\n",
    "output_names = [swa_key_pattern%(i) if (i+1)%6 != 0 else past_key_pattern%(i) for i in range(34)] + \\\n",
    "               [swa_value_pattern%(i) if (i+1)%6 != 0 else past_value_pattern%(i)  for i in range(34)]\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    swa_input_mod,\n",
    "    (past_key_inputs, past_value_inputs),\n",
    "    \"input-processor_swa_cl128_oga_rs.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    external_data = False,\n",
    "    opset_version=21\n",
    ")\n",
    "print(\"ONNX model 'input-processor_swa.onnx' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "211c93dc-1b72-4ce0-9553-2d645f8c1960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCKTest\\AppData\\Local\\Temp\\ipykernel_17108\\2663217263.py:26: UserWarning: Exporting a model while it is in training mode. Please ensure that this is intended, as it may lead to different behavior during inference. Calling model.eval() before export is recommended.\n",
      "  torch.onnx.export(\n",
      "W0212 10:51:26.021000 17108 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:110] torchvision is not installed. Skipping torchvision::nms\n",
      "W0212 10:51:26.023000 17108 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:110] torchvision is not installed. Skipping torchvision::roi_align\n",
      "W0212 10:51:26.024000 17108 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:110] torchvision is not installed. Skipping torchvision::roi_pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `InputSWAMod()` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `InputSWAMod()` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCKTest\\.pyenv\\pyenv-win\\versions\\3.12.10-arm\\Lib\\copyreg.py:99: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "ONNX model 'input-processor_swa.onnx' created successfully.\n"
     ]
    }
   ],
   "source": [
    "past_dim = context_length - 1\n",
    "swa_dim = 1023\n",
    "swa_input_mod = InputSWAMod(swa_dim, past_dim)\n",
    "\n",
    "past_key_shape = (4, 1, 256, past_dim)\n",
    "past_value_shape = (4, 1, past_dim, 256)\n",
    "\n",
    "past_key_inputs = [torch.zeros(past_key_shape, dtype = torch.uint8) for i in range(34)]\n",
    "past_value_inputs = [torch.zeros(past_value_shape, dtype = torch.uint8) for i in range(34)]\n",
    "\n",
    "output_key, output_value = swa_input_mod(past_key_inputs, past_value_inputs)\n",
    "\n",
    "own_past_key_pattern = \"own_past_key_%d_in\"\n",
    "own_past_value_pattern = \"own_past_value_%d_in\"\n",
    "\n",
    "past_key_pattern = \"past_key_%d_in\"\n",
    "past_value_pattern = \"past_value_%d_in\"\n",
    "swa_key_pattern = \"swa_key_%d_in\"\n",
    "swa_value_pattern = \"swa_value_%d_in\"\n",
    "\n",
    "input_names = [own_past_key_pattern%(i) for i in range(34)] + [own_past_value_pattern%(i) for i in range(34)]\n",
    "output_names = [swa_key_pattern%(i) if (i+1)%6 != 0 else past_key_pattern%(i) for i in range(34)] + \\\n",
    "               [swa_value_pattern%(i) if (i+1)%6 != 0 else past_value_pattern%(i)  for i in range(34)]\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    swa_input_mod,\n",
    "    (past_key_inputs, past_value_inputs),\n",
    "    \"input-processor_swa_cl1_oga_rs.onnx\",\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    external_data = False,\n",
    "    opset_version=21\n",
    ")\n",
    "print(\"ONNX model 'input-processor_swa.onnx' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db4b8f-1828-42ba-8204-a93ebe2e6bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b171f06d",
   "metadata": {},
   "source": [
    "# Output Processor Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "981e03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "270ad3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeChangeName(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        for input in inputs:\n",
    "            output = input.transpose(0, 1)\n",
    "            outputs.append(output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df04e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "changeNameMod = TransposeChangeName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7023c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.zeros((1, 4, 256, 128), dtype = torch.uint8) for i in range(34)] + \\\n",
    "            [torch.zeros((1, 4, 128, 256), dtype = torch.uint8) for i in range(34)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "231cbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = changeNameMod(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1ea82b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names=[\"past_key_%d_out\"%(i) if (i+1)%6 == 0 else \"swa_key_%d_out\"%(i) for i in range(34)] + \\\n",
    "                [\"past_value_%d_out\"%(i)  if (i+1)%6 == 0 else \"swa_value_%d_out\"%(i) for i in range(34)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c0aa42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names=[\"own_past_key_%d_out\"%(i) for i in range(34)] + [\"own_past_value_%d_out\"%(i) for i in range(34)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3334c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_axes = {}\n",
    "dynamic_axes.update({x: {3: \"kv_dim\"} if 'key' in x else {2: \"kv_dim\"} for x in input_names})\n",
    "dynamic_axes.update({x: {3: \"kv_dim\"} if 'key' in x else {2: \"kv_dim\"} for x in output_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5dd0e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HCKTest\\AppData\\Local\\Temp\\ipykernel_17108\\94929202.py:2: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter has become the default. Learn more about the new export logic: https://docs.pytorch.org/docs/stable/onnx_export.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html\n",
      "  torch.onnx.export(\n",
      "C:\\chilukam\\Gemma\\IOT\\artifacts_from_code\\gemma_env\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\utils.py:552: OnnxExporterWarning: Exporting to ONNX opset version 21 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 20. To use a newer opset version, consider 'torch.onnx.export(..., dynamo=True)'. \n",
      "  _export(\n"
     ]
    }
   ],
   "source": [
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    changeNameMod,\n",
    "    inputs,\n",
    "    \"transpose_outputs_cl_all.onnx\",\n",
    "    input_names = input_names,\n",
    "    output_names = output_names,\n",
    "    dynamic_axes = dynamic_axes,\n",
    "    opset_version=21,\n",
    "    dynamo = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6e812-de99-449a-b617-a17ea17bd142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab04750b-4aa9-487e-8113-adceb78a7c51",
   "metadata": {},
   "source": [
    "# Merge Output Processors Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b9ee59ea-3c09-4280-9c34-58a123645e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8a9dc20e-bb56-466f-9c67-7829035a78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_model = onnx.load(\"transpose_outputs_cl_all.onnx\")\n",
    "dq_model = onnx.load(\"dequantizer.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5dc7a0e4-841f-4de5-b753-3c3dda21a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = onnx.compose.merge_models(op_model, dq_model, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "73ccdea7-d0af-4897-8836-5533e59b20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(merged_model, \"output_processor.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b5c90-25b6-4549-a03e-a7ae437773d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183ed0cd",
   "metadata": {},
   "source": [
    "# OGA Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8b06a2c0-2a05-4b63-b421-b0d38ac03ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import json"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4e670d2-6b8c-45d9-9843-1aa2691abe7f",
   "metadata": {},
   "source": [
    "og.set_log_options(enabled=True, model_input_values=True, model_output_values=True,\n",
    "                  model_output_shapes=True, generate_next_token=True, append_next_tokens=True,\n",
    "                  model_logits=True, ort_lib=True, value_stats = True, ansi_tags = True, warning = True, hit_eos = True, hit_max_length = True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "33b4b733-f23f-42ba-a0c5-57de69d0cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2787bfe6-a6a1-42e9-a292-725ed1d7c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = og.Config(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ce4c1426-40b0-45d8-b63f-1d647463e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c9b5b695-f9f6-4dbc-8034-d6eb8cc1728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = og.Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "381cb81d-c126-4474-90ce-3c9e9f823d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = model.create_multimodal_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fdfa1f25-c757-4a69-b5c3-df5556329a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = processor.create_stream()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da286200-4e10-40a9-82f1-4696ce33f8b0",
   "metadata": {},
   "source": [
    "image_paths = [\"image_models/cat.jpg\"]\n",
    "images = og.Images.open(*image_paths)\n",
    "text = \"Briefly tell what is shown in this image?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9d549706-9f32-4032-aa25-fe25810f8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "images = None\n",
    "text = \"What are transformers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ae71e19e-8a83-4a6f-91be-cdad89098980",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f2e4fe48-de6f-4322-919f-0894a3a2476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [{\"type\": \"image\"} for _ in image_paths]\n",
    "content_list.append({\"type\": \"text\", \"text\": text})\n",
    "messages.append({\"role\": \"user\", \"content\": content_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "12b9f221-ec59-4de9-8071-900c1bc213d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_json = json.dumps(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ec38f221-aa4f-4bb3-a4e0-a31c57de2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(message_json, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f5e9e8bf-6492-44bf-913a-a4fe0c788558",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(prompt, images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "294d2ff8-2598-4200-99c8-65665b075260",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options(max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "06039b69-be16-4cb4-8be1-6394ae39d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = og.Generator(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "72930264-77b5-4d6d-aa5b-6a01ae3ea9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.set_inputs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29874306-2b41-4275-a42d-c7f2e12716ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e2078865-6d74-418c-975d-e298d0583bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "342b6825-be8f-4737-8331-b6f8fd7ca422",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_token = generator.get_next_tokens()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7488a002-1bb0-43e1-bfa6-6cc1d290ebda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay"
     ]
    }
   ],
   "source": [
    "print(stream.decode(new_token), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b6f55-5e1b-491f-9265-ef36c308599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2b9b1fcb-1ef7-4462-8151-1fe6173bf680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", let's break down what \"transformers\" are in the context of modern AI – specifically, large language models. They're a complex topic, but I'll explain it in a way that's both detailed and accessible.\n",
      "\n",
      "**Transformers: The Core Idea**\n",
      "\n",
      "At their heart, transformers are a type of computer program designed to understand and generate text.▁▁They've become *the* tool for many AI applications because they've proven exceptionally good at this.▁▁Let"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    if generator.is_done(): break\n",
    "    generator.generate_next_token()\n",
    "    new_token = generator.get_next_tokens()[0]\n",
    "    print(stream.decode(new_token), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d3897-173e-4f7d-bdc2-d5a079db3b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
