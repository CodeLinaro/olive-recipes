{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2138a0-3b03-4cd5-9d25-4d873de7ce49",
   "metadata": {},
   "source": [
    "# QNN Model Prepare on Linux\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK allows clients to run ML models on HTP hardware. The following steps describe how to prepare the Gemma3-4b models on Linux platforms for execution on Android.\n",
    "\n",
    "Before continuing, ensure all steps from [README](../../README.md) are completed. \n",
    "\n",
    "This document uses the term Qualcomm Neural Network (QNN) and Qualcomm AI Runtime SDK interchangeably.\n",
    "\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "1. Qualcomm AI Runtime SDK version 2.40.0\n",
    "2. Ubuntu 22.04 installation with required packages for QNN Tools\n",
    "3. Android Platform tools version 31 or greater\n",
    "4. This notebook could be executed with Anaconda (with the supplied environment.yaml) or a virtual environment(venv)\n",
    "5. Gemma3-4b language `.onnx` files and their corresponding AIMET encodings (generated via AIMET workflow)\n",
    "\n",
    "This work flow assumes that you have generated the Gemma3-4b model artifacts following the AIMET Gemma3-4b workflow (example1):\n",
    "\n",
    "- Gemma3-4b language model and its AIMET encodings\n",
    "- `*.pkl` files per network - numpy object array saved as a Python pickle that contains data that is required as part of the model conversion step.\n",
    "\n",
    "![dir_struct](../jupyter_notebook_assets/nb1_output_dir_contents.png \"Overall directory Structure from notebook 1\") ![onnx_dir_struct](../jupyter_notebook_assets/onnx_dir_struct.png \"Snapshot of file contents of onnx folder from notebook 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784c120-c7c0-487b-9482-077a9cd0854b",
   "metadata": {},
   "source": [
    "## Set up the Qualcomm AI Engine Direct SDK\n",
    "\n",
    "The following steps configure the Qualcomm AI Engine Direct SDK, which enables running Gemma3-4b on the device. \n",
    "Execute the following on an Ubuntu 22.04 terminal. \n",
    "\n",
    "**NOTE:** These steps require sudo or root privileges.\n",
    "\n",
    "1. After setting up Python and pip in Ubuntu, check QNN tool dependencies. \n",
    "2. Set the `QNN_SDK_ROOT` environment variable to the location of the Qualcomm AI Runtime Directory. For **Linux**, `export QNN_SDK_ROOT=\"./assets/qnn\"`\n",
    "3. Check and install Linux dependencies.\n",
    "\n",
    "    ```\n",
    "    source $QNN_SDK_ROOT/bin/check-linux-dependency.sh\n",
    "    sudo apt-get install -y libtinfo5\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdaad6-1020-41d5-b87f-6cbb961a7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set QNN_SDK_ROOT environment variable to the location of Qualcomm AI Engine Directory\n",
    "QNN_SDK_ROOT = '/tmp/qnn' # inset qnn path qnn2.31 \n",
    "# Check path QNN_SDK_ROOT\n",
    "assert os.path.exists(QNN_SDK_ROOT) == True,\"QNN_SDK_ROOT path does not exist\"\n",
    "os.environ['QNN_SDK_ROOT'] = QNN_SDK_ROOT\n",
    "print(QNN_SDK_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca84b5a-3e57-432f-b7d9-811c47cf71e4",
   "metadata": {},
   "source": [
    "### Install the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985358b6-d6a5-4468-9592-5c057b7a6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -r ../../example2_env_req.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e79a57-4622-4fd0-abb7-8be5b3bcdaa9",
   "metadata": {},
   "source": [
    "## Set up models and Qualcomm AI Engine Direct SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f1b22-57ee-4222-8fc5-0d329785bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import concurrent.futures\n",
    "import time\n",
    "from pathlib import Path\n",
    "# setup whether using multithread or single thread to compile\n",
    "go_parallel = True\n",
    "\n",
    "workfolder = os.getcwd()\n",
    "# Set up environment variable to reference GEMMA3_MODELS\n",
    "GEMMA3_MODELS ='/tmp/output_dir/export'\n",
    "print(GEMMA3_MODELS)\n",
    "# Check path GEMMA3_MODELS\n",
    "assert os.path.exists(GEMMA3_MODELS) == True,\"GEMMA3_MODELS path does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3734b-07dd-4cd2-a4f7-16adeaf044dd",
   "metadata": {},
   "source": [
    "# Workflow for Gemma3-4b models(language part of Gemma3-4b)\n",
    "\n",
    "\n",
    "All the models and encodings are processed independently via different executable QNN utilities available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "To prepare Gemma3-4b models(language part of Gemma3-4b) for inference, the QNN executable utilities require an Ubuntu 22.04 environment\n",
    "\n",
    "1. Split the onnx model into several small onnx models.\n",
    "2. Apply MHA2SHA transformation to convert all attention block MHAs to SHAs.\n",
    "3. Convert the `.onnx` files to their equivalent QNN representation.\n",
    "4. Generate the QNN model quantized libraries.\n",
    "5. Generate the QNN context binaries for the QNN HTP backend.\n",
    "\n",
    "After preparing the Gemma3-4b models(language part of Gemma3-4b) for inference, the next step is to execute the QNN context binaries for inference on a Snapdragon Android\n",
    "\n",
    "\n",
    "![QNN Work flow](../jupyter_notebook_assets/qnn-workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337599b-653d-47a7-8022-b9215b7d7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "sys.path.append(workfolder+'/../G2G')\n",
    "sys.path.append(workfolder+'/../G2G/split_onnx_utils')\n",
    "sys.path.append(workfolder+'/../../')\n",
    "from utilities.nsptargets import NspTargets\n",
    "from utilities.profiler import event_marker\n",
    "\n",
    "# Set up nsp target specification\n",
    "# Android GEN2 or Gen4 or higher is supported for this notebook\n",
    "nsp_target = NspTargets.Android.GEN2\n",
    "\n",
    "CL = 8192\n",
    "ARNs = [1, 128]\n",
    "\n",
    "EXPORT_AR = 473\n",
    "EXPORT_CONTEXT_LENGTH = 8192\n",
    "EXPORT_SLIDING_WINDOW_LENGTH = 1024\n",
    "SCL = 1024\n",
    "onnx_name = f\"gemma_4b\"\n",
    "num_splits = 4\n",
    "\n",
    "splits = range(1, num_splits+1)\n",
    "arn_list = [ arn for arn in ARNs for i in splits ]\n",
    "split_idxs = [i for arn in ARNs for i in splits]\n",
    "print('All task list:', [f\"ar{arn}-{n}\" for arn,n in zip(arn_list,split_idxs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab1139-c198-45c4-abc9-7087643fb546",
   "metadata": {},
   "source": [
    "# Prepare Gemma3-4b models(language part of Gemma3-4b) for Inference\n",
    "\n",
    "The following section uses the Qualcomm AI Runtime SDK to prepare Gemma3-4b models(language part of Gemma3-4b) for on-target inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bd34a-4114-49fc-8ada-57a8a3e5a58f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(f\"{workfolder}/assets/models_ar_n\", exist_ok=True)\n",
    "\n",
    "import change_hardcoding\n",
    "def gen_ar(arn):\n",
    "    try:\n",
    "        change_hardcoding.execute(\n",
    "                f\"{GEMMA3_MODELS}\",\n",
    "                f\"{workfolder}/assets/models_ar_n/ar{arn}-cl{CL}\",\n",
    "                [f\" {EXPORT_AR},{arn}\",\n",
    "                 f\" -{EXPORT_AR},-1\",\n",
    "                 f\" {EXPORT_CONTEXT_LENGTH},{CL}\",\n",
    "                 f\" {EXPORT_CONTEXT_LENGTH-EXPORT_AR},{CL-arn}\",\n",
    "                 f\" {EXPORT_SLIDING_WINDOW_LENGTH}, {SCL}\",\n",
    "                 f\" {EXPORT_SLIDING_WINDOW_LENGTH-EXPORT_AR}, {SCL-arn}\"\n",
    "                 ]\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "with event_marker(f'prepare-export'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(ARNs) if go_parallel else 1) as executor:\n",
    "        results = executor.map(gen_ar, ARNs)\n",
    "\n",
    "print(f\"Prepare AR128 AR1 export done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fdf9f-7ef0-422b-9eca-0c818751f277",
   "metadata": {},
   "source": [
    "## Preprocess ONNX \n",
    "\n",
    "Prior to utilizing the QNN tool chain to compile and generate the context binary for Gemma3-4b we need to split the model and generate the following artifacts\n",
    "- ONNX file for each split of the model\n",
    "- input vectors for each split\n",
    "- golden output vectors for each split\n",
    "\n",
    "We need to specify the following parameters to proceed with execution of the notebook and generate all necessary artifacts\n",
    "- number of splits of the model\n",
    "- path to Gemma3-4b onnx file\n",
    "- path to Gemma3-4b encodings file\n",
    "- path to *.pkl files \n",
    "  \n",
    "\n",
    "![Split](../jupyter_notebook_assets/ModelSplit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ddfaf-7c39-4b22-b00c-67aca0c6659f",
   "metadata": {},
   "source": [
    "### Set up environment variables for the Qualcomm AI Runtime SDK tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c07083-2b68-46d9-a6be-5f3c8face599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "\n",
    "qnn_env = os.environ.copy()\n",
    "qnn_env[\"QNN_SDK_ROOT\"] = QNN_SDK_ROOT\n",
    "qnn_env[\"PYTHONPATH\"] = QNN_SDK_ROOT + \"/benchmarks/QNN/:\" + QNN_SDK_ROOT + \"/lib/python\"\n",
    "qnn_env[\"PATH\"] = QNN_SDK_ROOT + \"/bin/x86_64-linux-clang:\" + qnn_env[\"PATH\"]\n",
    "qnn_env[\"LD_LIBRARY_PATH\"] = QNN_SDK_ROOT + \"/lib/x86_64-linux-clang\"\n",
    "qnn_env[\"HEXAGON_TOOLS_DIR\"] = QNN_SDK_ROOT + \"/bin/x86_64-linux-clang\"\n",
    "# qnn_env[\"NUM_LAYERS_PER_SPLIT\"] = \"28\"\n",
    "qnn_env[\"LLM\"] = \"1\"\n",
    "qnn_env[\"split_embedding\"] = \"0\"\n",
    "qnn_env[\"split_lmhead\"] = \"0\"\n",
    "os.environ = qnn_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab80db6-ed85-4aba-ad4f-3e2f975751e9",
   "metadata": {},
   "source": [
    "### Split Onnx export\n",
    "\n",
    "This step splits a model into multiple parts based on the number of splits specified.\n",
    "\n",
    "Expected execution time: ~< 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f557c-e531-47fc-ae32-952566b9c93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_split(arn):\n",
    "    try:\n",
    "        name = f\"ar{arn}-cl{CL}\"\n",
    "        model_export = f\"{workfolder}/assets/models_ar_n\"\n",
    "        model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "        os.makedirs(model_artifact, exist_ok = True)\n",
    "    \n",
    "        # create symlink to export\n",
    "        symlink_src = os.path.join(model_artifact, 'src')\n",
    "        symlink_path = Path(symlink_src)\n",
    "        if symlink_path.is_symlink():\n",
    "            os.unlink(symlink_src)\n",
    "        os.symlink(src = os.path.join(model_export, name), dst = symlink_src)\n",
    "    \n",
    "        os.makedirs(f\"{model_artifact}/split_onnx\", exist_ok = True)\n",
    "        TEST_VECTOR_PICKLE_TYPE = \"pkl\"\n",
    "        print(f\"Starting {onnx_name}.onnx\")\n",
    "        utils.split_onnx(onnxfile = f\"{model_artifact}/src/onnx/{onnx_name}.onnx\", modelname = name,\n",
    "                        pickle_filedir = os.path.join(model_export, f\"ar{arn}-cl{CL}/test_vectors\"),\n",
    "                        num_splits = num_splits, output_dir = model_artifact, split_embedding = False,\n",
    "                        encoding_file = f\"{model_artifact}/src/onnx/{onnx_name}.encodings\",using_qairt_workflow = True\n",
    "                        )\n",
    "        print(f\"Ending {onnx_name}.onnx\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "with event_marker(f'split-onnx'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(ARNs) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_split, ARNs)\n",
    "\n",
    "print(f\"All onnx model splitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e5ccb-4550-4efe-9d97-e1512aea9877",
   "metadata": {},
   "source": [
    "### Convert attention layers from MHA to SHA\n",
    "\n",
    "The `mha2sha-onnx-converter` tool converts a model from MHA representation to its equivalent SHA representation. The encoding files generated from the AIMET workflow are provided as an input to this step via the `--exported-model-encoding-path` option.\n",
    "\n",
    "This step generates a new `.onnx` file that represents the model in SHA format.\n",
    "\n",
    "Expected execution time: ~20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9cbfd-f6b4-4049-b9fa-6e042f830d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mha2sha_root = workfolder+\"/../G2G/MHA2SHA\"\n",
    "g2g_env = os.environ.copy()\n",
    "g2g_env[\"PYTHONPATH\"] = os.pathsep.join([g2g_env.get(\"PYTHONPATH\", \"\"), os.path.join(mha2sha_root, \"src/python\")])\n",
    "g2g_env[\"PATH\"] = os.pathsep.join([g2g_env.get(\"PATH\", \"\"), os.path.join(mha2sha_root, \"bin\")])\n",
    "\n",
    "print(f\"MHA2SHA tool root set to: {mha2sha_root}\")\n",
    "\n",
    "def thread_g2g(arn,split):\n",
    "    try:\n",
    "        model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "        split_work_dir = os.path.join(model_artifact,f\"{split}_of_{num_splits}\")\n",
    "        name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "        os.makedirs(split_work_dir, exist_ok = True)\n",
    "        sha_folder = f\"{split_work_dir}/sha_output/\"\n",
    "        os.makedirs(sha_folder, exist_ok = True)\n",
    "        name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "        print(f\"mha2sha-onnx-converter {name} running...\")\n",
    "\n",
    "        sys.path.insert(0, QNN_SDK_ROOT + \"/lib/python\")\n",
    "        from qti.aisw.tools.core.utilities.framework.frameworks.onnx.onnx_model import OnnxModel\n",
    "\n",
    "\n",
    "        onnxmodel = OnnxModel.load(\n",
    "            model_path=f\"{model_artifact}/split_onnx/{name}.onnx\",\n",
    "            encodings_path=f\"{model_artifact}/src/onnx/{onnx_name}.encodings\"\n",
    "        )\n",
    "        # Run mha2sha\n",
    "        onnxmodel.mha2sha_v2()\n",
    "        # Save output\n",
    "        onnxmodel.export(str(sha_folder), prefix=name)\n",
    "\n",
    "        print(f\"mha2sha-onnx-converter {name} done.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "with event_marker(f'mha2sha'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(arn_list) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_g2g, arn_list, split_idxs)\n",
    "\n",
    "print(f\"All mha2sha convert done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926eed57-9b14-47d6-ac1f-92fd2be45712",
   "metadata": {},
   "source": [
    "## Convert the model from ONNX representation to QNN DLC representation\n",
    "\n",
    "The Qualcomm AI Engine Direct SDK `qairt-converter` tool converts a model from ONNX representation to its equivalent QNN DLC representation. The encoding files generated from the AIMET workflow are provided as an input to this step via the `â€“quantization_overrides model.encodings` option.\n",
    "\n",
    "This step generates a `.dlc` file that represents the model as a series of QNN API calls.\n",
    "\n",
    "Expected execution time: ~< 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9446e-9bcc-4ff3-83e2-96a2b1468fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_convert(arn,split):\n",
    "    try:\n",
    "        model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "        split_work_dir = os.path.join(model_artifact,f\"{split}_of_{num_splits}\")\n",
    "        name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "        os.makedirs(split_work_dir, exist_ok = True)\n",
    "        out_dir = os.path.join(split_work_dir, \"converted_model\")\n",
    "        os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "        # create symlink to export\n",
    "        for src in [f\"input_list_{name}.txt\",f\"test_inputs_{name}\"]:\n",
    "            symlink_input = os.path.join(split_work_dir, src)\n",
    "            symlink_path = Path(symlink_input)\n",
    "            if symlink_path.is_symlink():\n",
    "                os.unlink(symlink_input)\n",
    "            os.symlink(src = os.path.join(model_artifact, src), dst = symlink_input)\n",
    "        input_onnx=f\"{split_work_dir}/sha_output/{name}.onnx\"\n",
    "        quantization_overrides= f\"{split_work_dir}/sha_output/{name}.encodings\"\n",
    "\n",
    "        args = [QNN_SDK_ROOT + \"/bin/x86_64-linux-clang/qairt-converter\",\n",
    "                        \"--input_network\", input_onnx,\n",
    "                        \"--quantization_overrides\", quantization_overrides,\n",
    "                        \"-o\", f'{out_dir}/{name}.dlc'\n",
    "                        ]\n",
    "\n",
    "        options = utils.get_input_layout(input_onnx, using_qairt_workflow = True)\n",
    "        for entry in options:\n",
    "            args+=entry\n",
    "\n",
    "        proc = subprocess.Popen(args, stdout = subprocess.PIPE, stderr = subprocess.PIPE, env = qnn_env)\n",
    "        output, error = proc.communicate()\n",
    "        print(output.decode(), error.decode())\n",
    "        print(f\"qairt-converter {name} done!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "with event_marker(f'convert-onnx'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(split_idxs) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_convert, arn_list, split_idxs)\n",
    "\n",
    "print(f\"All qairt-converter done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac569d09-c507-4dd2-b33e-00a388e2339c",
   "metadata": {},
   "source": [
    "##  Quantized QNN DLC model\n",
    "\n",
    "The  Qualcomm AI Engine Direct SDK `qairt-quantizer` compiles the model `.dlc` and input`.raw` files into a `model.quantized.dlc` file.\n",
    "\n",
    "The inputs to this stage are the input raw files &  `model.dlc` generated in the previous step.\n",
    "\n",
    "Expected execution time: ~< 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15294f-507e-4e5d-a288-7c6665617742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def thread_genlib(arn,split):\n",
    "    try:\n",
    "        model_artifact = f\"{workfolder}/assets/artifacts/ar{arn}-cl{CL}/\"\n",
    "        split_work_dir = os.path.join(model_artifact,f\"{split}_of_{num_splits}\")\n",
    "        name = f\"ar{arn}-cl{CL}_{split}_of_{num_splits}\"\n",
    "        os.chdir(split_work_dir)\n",
    "        out_dir = os.path.join(split_work_dir,\"compiled_model\")\n",
    "        os.makedirs( os.path.join(split_work_dir,\"compiled_model\"), exist_ok = True)\n",
    "\n",
    "        float_dlc_file = os.path.join(split_work_dir, \"converted_model\", f'{name}.dlc')\n",
    "        quantized_dlc_file = os.path.join(out_dir, f'{name}_quantized.dlc')\n",
    "        ip_list_file = os.path.join(model_artifact, f'input_list_{name}.txt')\n",
    "\n",
    "        proc = subprocess.Popen([QNN_SDK_ROOT + \"/bin/x86_64-linux-clang/qairt-quantizer\",\n",
    "                                \"--input_dlc\", float_dlc_file,\n",
    "                                \"--input_list\", ip_list_file,\n",
    "                                \"--output_dlc\", quantized_dlc_file,\n",
    "                                \"--act_bitwidth\", \"16\",\n",
    "                                \"--bias_bitwidth\", \"32\"\n",
    "                                ],stdout = subprocess.PIPE, stderr = subprocess.PIPE, env = qnn_env)\n",
    "        output, error = proc.communicate()\n",
    "        print(output.decode(), error.decode())\n",
    "        print(f\"qairt-quantizer {name} done!\")\n",
    "        os.chdir(workfolder)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "with event_marker(f'qairt-quantizer'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(split_idxs) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_genlib, arn_list, split_idxs)\n",
    "\n",
    "print(f\"All qairt-quantizer done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ed1a2-7f66-4f36-a21f-0794246bff18",
   "metadata": {},
   "source": [
    "## QNN HTP weight sharing context binary\n",
    "\n",
    "The  Qualcomm AI Engine Direct SDK `qnn-context-binary-generator` tool creates a QNN context binary applicable to the QNN HTP backend. This binary can be deployed to run on a Snapdragon 8 Gen4 device that runs Android. This step requires the ar128 and ar1 quantized DLCs from the previous step and the `libQnnHtp.so` library, available in the Qualcomm AI Runtime SDK.\n",
    "\n",
    "Provide additional options that pertain to the QNN HTP backend by passing the `libQnnHtpBackendExtensions.so` library that implements extensions for the QNN HTP backend. The library is available in the Qualcomm AI Engine Direct SDK.\n",
    "\n",
    "### Define Htp Perf Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c21b3b-937d-4db6-ac14-af879aa4d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def make_config_file(index, folder, src_graphs, soc_id=43, dsp_arch=\"v73\"):\n",
    "    htp_config_json = os.path.join(folder, f\"HtpConfigFile_API_{index}.json\")\n",
    "    perf_config_json = os.path.join(folder, f\"PerfSetting_API_{index}.conf\")\n",
    "\n",
    "    soc_id = int(soc_id)\n",
    "    with open(htp_config_json, 'w') as f:\n",
    "        config = {\n",
    "            \"backend_extensions\": {\n",
    "                \"shared_library_path\": \"libQnnHtpNetRunExtensions.so\",\n",
    "                \"config_file_path\": f\"{perf_config_json}\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    with open(perf_config_json,'w') as f:\n",
    "        config = {\n",
    "            \"graphs\": [{\n",
    "                \"O\": 3.0,\n",
    "                \"vtcm_mb\": 8,\n",
    "                \"graph_names\": src_graphs,\n",
    "                \"fp16_relaxed_precision\": 0,\n",
    "                \"hvx_threads\": 8\n",
    "            }],\n",
    "            \"devices\": [\n",
    "                {\n",
    "                    \"soc_id\": soc_id,\n",
    "                    \"dsp_arch\": dsp_arch,\n",
    "                    \"cores\": [\n",
    "                        {\n",
    "                            \"perf_profile\": \"burst\",\n",
    "                            \"rpc_control_latency\": 100\n",
    "                        }\n",
    "                    ],\n",
    "                    \"pd_session\": \"unsigned\"\n",
    "                }\n",
    "            ],\n",
    "            \"context\": {\n",
    "                    \"extended_udma\": True,\n",
    "                    \"weight_sharing_enabled\": len(src_graphs) > 1\n",
    "            },\n",
    "            \"memory\": {\n",
    "                    \"mem_type\": \"shared_buffer\"\n",
    "            }\n",
    "        }\n",
    "        json.dump(config, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff5cf9-f9ec-413c-8049-b4e740ab0189",
   "metadata": {},
   "source": [
    "### Compile context binary\n",
    "Expected execution time: ~20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b004dff-4f4c-4671-9d9a-a35548c14b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "soc_id = 88\n",
    "dsp_arch ='v81'\n",
    "\n",
    "def thread_gen_ws_cb(i):\n",
    "    try:\n",
    "        ar128_src = f\"{workfolder}/assets/artifacts/ar128-cl{CL}/\"\n",
    "        ar1_src = f\"{workfolder}/assets/artifacts/ar1-cl{CL}/\"\n",
    "        output_dir = f\"{workfolder}/assets/artifacts/ar128-ar1-cl{CL}_conf_files/\"\n",
    "        ctx_output_dir = f\"{workfolder}/assets/artifacts/ar128-ar1-cl{CL}/\"\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(ctx_output_dir, exist_ok = True)\n",
    "\n",
    "        src1_split_folder = os.path.join(ar128_src, f\"{i}_of_{num_splits}\", \"compiled_model\")\n",
    "        src2_split_folder = os.path.join(ar1_src, f\"{i}_of_{num_splits}\", \"compiled_model\")\n",
    "\n",
    "        src1_graph_name = f\"ar128-cl{CL}_{i}_of_{num_splits}\"\n",
    "        src1_q_dlc = os.path.join(src1_split_folder, f\"{src1_graph_name}_quantized.dlc\")\n",
    "        src2_graph_name = f\"ar1-cl{CL}_{i}_of_{num_splits}\"\n",
    "        src2_q_dlc = os.path.join(src2_split_folder, f\"{src2_graph_name}_quantized.dlc\")\n",
    "\n",
    "        graph_list = [src1_graph_name, src2_graph_name]\n",
    "        make_config_file(i, output_dir, graph_list, soc_id, dsp_arch)\n",
    "\n",
    "        cmd = [\"qnn-context-binary-generator\",\n",
    "                \"--log_level=verbose\",\n",
    "                \"--backend\",\"libQnnHtp.so\",\n",
    "                \"--model\", \"libQnnModelDlc.so\",\n",
    "                \"--input_output_tensor_mem_type\", \"memhandle\",\n",
    "                \"--output_dir\", ctx_output_dir,\n",
    "                \"--config_file\",f\"{output_dir}/HtpConfigFile_API_{i}.json\",\n",
    "                \"--binary_file\", f\"weight_sharing_model_{i}_of_{num_splits}.serialized\",\n",
    "                \"--dlc_path\", f\"{src1_q_dlc},{src2_q_dlc}\"]\n",
    "        \n",
    "        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=qnn_env)\n",
    "        output, error = proc.communicate()\n",
    "        print(output.decode(), error.decode())\n",
    "        print(f'#{i} weight sharing model generated')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "with event_marker(f'gen-binary'):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers = len(splits) if go_parallel else 1) as executor:\n",
    "        results = executor.map(thread_gen_ws_cb, splits)\n",
    "\n",
    "print(f\"All weight shared qnn-context-binary generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781a4ab-4bf6-47d1-afa1-808fe193059e",
   "metadata": {},
   "source": [
    "### Save profiling stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f3f37-6df0-48e8-88fa-fbec926221b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.profiler import EventProfiler\n",
    "EventProfiler().report()\n",
    "EventProfiler().json_dump(os.path.join(workfolder, 'assets/profiling_stats.json'))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
